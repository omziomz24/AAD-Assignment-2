% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Group-26 AAD-Assignment-2},
  pdfauthor={Omar, Eloise, Alina, Sue},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Group-26 AAD-Assignment-2}
\author{Omar, Eloise, Alina, Sue}
\date{2025-05-20}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{4}
\tableofcontents
}
\section{2.1 Nonlinear model vs non-parametric
model}\label{nonlinear-model-vs-non-parametric-model}

\subsection{2.1.1 GAM vs KNN approach}\label{gam-vs-knn-approach}

\noindent \large \textbf{GAM:} \normalsize

\noindent \textbf{Pros}

\begin{itemize}
  \item \textbf{Flexibility to capture non-linearity}. In a GAMM we replace each linear term $\beta_{k} x_{k}$ with a smooth function $f_{k}(x_{k})$,     meaning we can automatically model non-linear relationships between each predictor and the response without having to manually try out different      transformation on each variable individually.
  \item \textbf{Non-linear fits may be more accurate}. The ability of creating this non-linear predictor and response relationship may make our model     more accurate when predicting the medianHousingValue.
  \item \textbf{Interperability from additivity}. Since a GAMM is additive, we can examine the effect of each $X_j$ on $Y$ individually while holding      all of the other variables fixed. Therefore, we can understand each variables individual effect on house value.
  \item \textbf{Control over smoothness}. Each $f_j$ comes with an associated smoothing parameter (or degrees of freedom), making it straightforward      to trade bias and variance  (e.g. via cross-validation).
\end{itemize}

\noindent \textbf{Cons}

\begin{itemize}
  \item \textbf{Additivity assumption restriction}. If there are strong synergistic effects like between location (longitude/latitude) and income         \textendash an additive model will not capture them unless explicit interaction terms $X_j \times X_k$ are included or low-dimensional               interactions function of the form $f_{jk}(X_j, X_k)$ are manually introduced. 
  \item \textbf{Computational cost}. Fitting this model to over \textasciitilde$20,600$ observations and multiple smoothers can be very slow when        compared to fitting a single parametric method.
\end{itemize}

\noindent \large \textbf{KNN:} \normalsize

\noindent \textbf{Pros}

\begin{itemize}
  \item \textbf{Completely non-parametric}. KNN makes no assumptions about the form of $f(X)$, allowing the model to potentially fit better than a      parametric model. At a point $x_0$ KNN averages the responses of the $K$ closest training blocks: $\hat{f}(x_{0}) = \frac{1}{K} \sum_{x_{i}           \in N_{0}}{y_i}$, where $N_0$ is the set of the $K$ nearest neighbors.
  \item \textbf{Control of bias-variance}. A small $K$ yields a very flexible, low-bias but high-variance fit; large $K$ yields a smoother,      lower-variance fit.
\end{itemize}

\noindent \textbf{Cons}

\begin{itemize}
  \item \textbf{Dimensoniality constraints}. As the number of predictors grows, the "nearest" neighbors tend to be far away in a high-dimensional       space, so KNN's performance degrades rapidly as the number of predictors grow.
  \item \textbf{Distance metric sensitivity}. With a KNN you must scale the numeric features and encode the categorical features (e.g.                  oceanProximity) carefully. Otherwise poorly scaled or encoded features can dominate the distance of the nearest neighbors calculation.
  \item \textbf{Computationally intensive with predictions}. For each new group of predictions all \textasciitilde$20,600$ must be computed to find     the $K$ nearest, which can be very intensive and slow.
  \item \textbf{Low interperability}. There is no simple way to explain a KNN prediction beyond pointing to the raw neighbors and their average.
\end{itemize}

\noindent \large \textbf{In this housing-price context: } \normalsize

\begin{itemize}
  \item \textbf{GAMM:} it is likely to give an interpretable model with, as we can analyse the partial-effects (e.g. how median income or ocean         proximity individually affects price), and we can capture smooth non-linear trends.
  \item \textbf{KNN:} can capture complex interactions, between predictors, automatically, but with eight predictors (including a categorical     one)   it may run into high-dimensionality issues, making distance-based averaging unstable and slow.
\end{itemize}

\subsection{2.1.2 GAM vs KNN Regression
Model}\label{gam-vs-knn-regression-model}

We will use the cleaned data (omitting all data point that contains
``NA'') instead of the original raw dataset. We split the data into a
training set (80\% of the original data) and a test set (20\% of the
original data).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load the data }
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"Assignt2\_data.csv"}\NormalTok{)}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# separate the training and test set out }
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{train\_index }\OtherTok{\textless{}{-}} \FunctionTok{createDataPartition}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{medianHouseValue, }\AttributeTok{p =} \FloatTok{0.8}\NormalTok{, }\AttributeTok{list =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{train\_data }\OtherTok{\textless{}{-}}\NormalTok{ data[train\_index, ]}
\NormalTok{test\_data  }\OtherTok{\textless{}{-}}\NormalTok{ data[}\SpecialCharTok{{-}}\NormalTok{train\_index, ]}
\end{Highlighting}
\end{Shaded}

\subsubsection{Basic KNN Regression
Model}\label{basic-knn-regression-model}

The basic KNN regression model was trained using 5-fold cross-validation
on the training dataset, with k tuned from 1 to 10. Preprocessing steps
included centering and scaling to ensure features contribute equally to
the distance metric.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Basic KNN}

\NormalTok{knn\_reg\_model\_1 }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(}
\NormalTok{  medianHouseValue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
  \AttributeTok{data =}\NormalTok{ train\_data,}
  \AttributeTok{method =} \StringTok{"knn"}\NormalTok{,}
  \AttributeTok{preProcess =} \FunctionTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \AttributeTok{tuneGrid =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{k =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{),  }\CommentTok{\# test k = 1 to 10}
  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{5}\NormalTok{) }
\NormalTok{) }

\CommentTok{\# best k }
\FunctionTok{print}\NormalTok{(knn\_reg\_model\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## k-Nearest Neighbors 
## 
## 16348 samples
##     9 predictor
## 
## Pre-processing: centered (12), scaled (12) 
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 13080, 13078, 13077, 13079, 13078 
## Resampling results across tuning parameters:
## 
##   k   RMSE      Rsquared   MAE     
##    1  78263.78  0.5866013  51134.82
##    2  68539.95  0.6576558  45714.66
##    3  65188.16  0.6840453  43863.05
##    4  63511.49  0.6981273  42904.46
##    5  62763.65  0.7044630  42476.71
##    6  62320.09  0.7082957  42184.58
##    7  62295.64  0.7084690  42252.06
##    8  62324.43  0.7082763  42194.31
##    9  62180.58  0.7096554  42039.43
##   10  62165.35  0.7098677  42030.93
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was k = 10.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# KNN MSE calculation }
\NormalTok{knn\_pred\_1 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(knn\_reg\_model\_1, }\AttributeTok{newdata =}\NormalTok{ test\_data)}
\NormalTok{knn\_MSE\_1 }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((knn\_pred\_1 }\SpecialCharTok{{-}}\NormalTok{ test\_data}\SpecialCharTok{$}\NormalTok{medianHouseValue)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }
\FunctionTok{cat}\NormalTok{(}\StringTok{"The test MSE of basic KNN model is"}\NormalTok{, knn\_MSE\_1, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## The test MSE of basic KNN model is 3697939911
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The KNN MSE is 3697939911 }
\end{Highlighting}
\end{Shaded}

The test MSE of basic KNN model is 3,697,939,911

\subsubsection{KNN Model Improvements}\label{knn-model-improvements}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Using the predictors from Asmt1 ? }
\NormalTok{data\_imp }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"Assignt2\_data.csv"}\NormalTok{)}
\NormalTok{data\_imp }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(data\_imp)}

\CommentTok{\# bedroomsPerRoom}
\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{bedroomsPerRoom }\OtherTok{\textless{}{-}}\NormalTok{ data\_imp}\SpecialCharTok{$}\NormalTok{aveBedrooms }\SpecialCharTok{/}\NormalTok{ data\_imp}\SpecialCharTok{$}\NormalTok{aveRooms}


\CommentTok{\# incomePerRoom}
\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{incomePerRoom }\OtherTok{=}\NormalTok{ data\_imp}\SpecialCharTok{$}\NormalTok{medianIncome }\SpecialCharTok{/}\NormalTok{ data\_imp}\SpecialCharTok{$}\NormalTok{aveRooms}

\CommentTok{\# Compute distances}
\NormalTok{la\_coords }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{118.24}\NormalTok{, }\FloatTok{34.05}\NormalTok{)   }\CommentTok{\# (longitude, latitude)}
\NormalTok{sf\_coords }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{122.42}\NormalTok{, }\FloatTok{37.77}\NormalTok{)}

\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{distToLA }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(data\_imp[, }\FunctionTok{c}\NormalTok{(}\StringTok{"longitude"}\NormalTok{, }\StringTok{"latitude"}\NormalTok{)], }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(coord) \{}
  \FunctionTok{distGeo}\NormalTok{(coord, la\_coords) }\SpecialCharTok{/} \DecValTok{1000}
\NormalTok{\})}
\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{distToSF }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(data\_imp[, }\FunctionTok{c}\NormalTok{(}\StringTok{"longitude"}\NormalTok{, }\StringTok{"latitude"}\NormalTok{)], }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(coord) \{}
  \FunctionTok{distGeo}\NormalTok{(coord, sf\_coords) }\SpecialCharTok{/} \DecValTok{1000}
\NormalTok{\})}

\CommentTok{\# Compute directions}
\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{dirToLA }\OtherTok{\textless{}{-}} \FunctionTok{atan2}\NormalTok{(data\_imp}\SpecialCharTok{$}\NormalTok{latitude }\SpecialCharTok{{-}}\NormalTok{ la\_coords[}\DecValTok{2}\NormalTok{], data\_imp}\SpecialCharTok{$}\NormalTok{longitude }\SpecialCharTok{{-}}\NormalTok{ la\_coords[}\DecValTok{1}\NormalTok{])}
\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{dirToSF }\OtherTok{\textless{}{-}} \FunctionTok{atan2}\NormalTok{(data\_imp}\SpecialCharTok{$}\NormalTok{latitude }\SpecialCharTok{{-}}\NormalTok{ sf\_coords[}\DecValTok{2}\NormalTok{], data\_imp}\SpecialCharTok{$}\NormalTok{longitude }\SpecialCharTok{{-}}\NormalTok{ sf\_coords[}\DecValTok{1}\NormalTok{])}

\CommentTok{\# Cosine and sine of direction}
\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{cosDirToLA }\OtherTok{\textless{}{-}} \FunctionTok{cos}\NormalTok{(data\_imp}\SpecialCharTok{$}\NormalTok{dirToLA)}
\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{sinDirToLA }\OtherTok{\textless{}{-}} \FunctionTok{sin}\NormalTok{(data\_imp}\SpecialCharTok{$}\NormalTok{dirToLA)}
\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{cosDirToSF }\OtherTok{\textless{}{-}} \FunctionTok{cos}\NormalTok{(data\_imp}\SpecialCharTok{$}\NormalTok{dirToSF)}
\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{sinDirToSF }\OtherTok{\textless{}{-}} \FunctionTok{sin}\NormalTok{(data\_imp}\SpecialCharTok{$}\NormalTok{dirToSF)}
\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{dirToLA }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{dirToSF }\OtherTok{\textless{}{-}} \ConstantTok{NULL}

\CommentTok{\# Composite proximity score}
\NormalTok{data\_imp}\SpecialCharTok{$}\NormalTok{cityProximityScore }\OtherTok{\textless{}{-}} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ data\_imp}\SpecialCharTok{$}\NormalTok{distToLA) }\SpecialCharTok{+} \DecValTok{1} \SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ data\_imp}\SpecialCharTok{$}\NormalTok{distToSF)}

\CommentTok{\# Add these to test and train}
\NormalTok{train\_data\_imp }\OtherTok{\textless{}{-}}\NormalTok{ data\_imp[train\_index, ]}
\NormalTok{test\_data\_imp  }\OtherTok{\textless{}{-}}\NormalTok{ data\_imp[}\SpecialCharTok{{-}}\NormalTok{train\_index, ]}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# KNN with model improvements}
\CommentTok{\# Hot encoding the variables}

\CommentTok{\# Convert oceanProximity to factor (if not already)}
\NormalTok{train\_data\_imp}\SpecialCharTok{$}\NormalTok{oceanProximity }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(train\_data\_imp}\SpecialCharTok{$}\NormalTok{oceanProximity)}
\NormalTok{test\_data\_imp}\SpecialCharTok{$}\NormalTok{oceanProximity }\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(test\_data\_imp}\SpecialCharTok{$}\NormalTok{oceanProximity)}

\CommentTok{\# One{-}hot encode using dummyVars}
\NormalTok{dummies }\OtherTok{\textless{}{-}} \FunctionTok{dummyVars}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ train\_data\_imp)}
\NormalTok{train\_data\_encoded }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(dummies, }\AttributeTok{newdata =}\NormalTok{ train\_data\_imp)}
\NormalTok{test\_data\_encoded }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(dummies, }\AttributeTok{newdata =}\NormalTok{ test\_data\_imp)}

\CommentTok{\# Convert to data frame}
\NormalTok{train\_data\_encoded }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(train\_data\_encoded)}
\NormalTok{test\_data\_encoded }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(test\_data\_encoded)}



\NormalTok{knn\_reg\_model\_2 }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(}
\NormalTok{  medianHouseValue }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
  \AttributeTok{data =}\NormalTok{ train\_data\_encoded,}
  \AttributeTok{method =} \StringTok{"knn"}\NormalTok{,}
  \AttributeTok{preProcess =} \FunctionTok{c}\NormalTok{(}\StringTok{"center"}\NormalTok{, }\StringTok{"scale"}\NormalTok{),}
  \AttributeTok{tuneGrid =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{k =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{15}\NormalTok{),  }\CommentTok{\# test k = 1 to 15}
  \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{5}\NormalTok{) }
\NormalTok{) }

\CommentTok{\# best k }
\FunctionTok{print}\NormalTok{(knn\_reg\_model\_2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## k-Nearest Neighbors 
## 
## 16348 samples
##    22 predictor
## 
## Pre-processing: centered (22), scaled (22) 
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 13078, 13080, 13078, 13078, 13078 
## Resampling results across tuning parameters:
## 
##   k   RMSE      Rsquared   MAE     
##    1  66626.83  0.6881727  42322.38
##    2  59415.37  0.7396997  38448.18
##    3  56662.10  0.7603038  36864.82
##    4  55245.10  0.7712116  36223.52
##    5  54703.40  0.7754714  35869.36
##    6  54188.49  0.7796901  35662.06
##    7  54127.21  0.7802974  35691.29
##    8  54026.89  0.7812213  35623.50
##    9  53980.31  0.7817264  35644.96
##   10  54059.94  0.7812382  35717.07
##   11  54201.81  0.7802371  35862.91
##   12  54310.24  0.7795122  35971.60
##   13  54433.91  0.7786415  36096.84
##   14  54596.19  0.7774474  36225.05
##   15  54782.09  0.7760042  36327.64
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was k = 9.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# KNN MSE calculation }
\NormalTok{knn\_pred\_2 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(knn\_reg\_model\_2, }\AttributeTok{newdata =}\NormalTok{ test\_data\_encoded)}
\NormalTok{knn\_MSE\_2 }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((knn\_pred\_2 }\SpecialCharTok{{-}}\NormalTok{ test\_data\_encoded}\SpecialCharTok{$}\NormalTok{medianHouseValue)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }
\FunctionTok{cat}\NormalTok{(}\StringTok{"The KNN MSE (improved) is"}\NormalTok{, knn\_MSE\_2, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## The KNN MSE (improved) is 2980003191
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#The KNN MSE is 3697939911 with out encoding}
\CommentTok{\#With encoding  3694531899}
\CommentTok{\#PCA makes it worse in general}
\CommentTok{\#improve when tested more k 3650562433}
\CommentTok{\# best now is: 2999761722 }
\end{Highlighting}
\end{Shaded}

\subsubsection{Basic GAM}\label{basic-gam}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# GAM }
\CommentTok{\# the split is the same as before }

\CommentTok{\# train the GAM }
\NormalTok{gam\_model\_1 }\OtherTok{\textless{}{-}}\NormalTok{ mgcv}\SpecialCharTok{::}\FunctionTok{gam}\NormalTok{(medianHouseValue }\SpecialCharTok{\textasciitilde{}} \FunctionTok{s}\NormalTok{(latitude) }\SpecialCharTok{+} \FunctionTok{s}\NormalTok{(longitude) }\SpecialCharTok{+} 
                   \FunctionTok{s}\NormalTok{(housingMedianAge) }\SpecialCharTok{+} \FunctionTok{s}\NormalTok{(aveRooms) }\SpecialCharTok{+}
                   \FunctionTok{s}\NormalTok{(aveBedrooms) }\SpecialCharTok{+} \FunctionTok{s}\NormalTok{(population) }\SpecialCharTok{+}
                   \FunctionTok{s}\NormalTok{(medianIncome) }\SpecialCharTok{+} \FunctionTok{factor}\NormalTok{(oceanProximity), }\AttributeTok{data =}\NormalTok{ train\_data)}

\CommentTok{\# Calculate MSE }
\NormalTok{gam\_pred\_1 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(gam\_model\_1, }\AttributeTok{newdata =}\NormalTok{ test\_data)}
\NormalTok{gam\_MSE\_1 }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((gam\_pred\_1 }\SpecialCharTok{{-}}\NormalTok{ test\_data}\SpecialCharTok{$}\NormalTok{medianHouseValue)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"The GAM MSE is"}\NormalTok{, gam\_MSE\_1, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## The GAM MSE is 4258008553
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#4258008383 }
\end{Highlighting}
\end{Shaded}

\subsubsection{GAM Improvements}\label{gam-improvements}

some justifications required:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# GAM improvements}
\NormalTok{train\_data\_imp }\OtherTok{=}\NormalTok{ data\_imp[train\_index, ]}
\NormalTok{gam\_model\_2 }\OtherTok{\textless{}{-}}\NormalTok{ mgcv}\SpecialCharTok{::}\FunctionTok{gam}\NormalTok{(medianHouseValue }\SpecialCharTok{\textasciitilde{}} 
                     \FunctionTok{s}\NormalTok{(housingMedianAge) }\SpecialCharTok{+} 
                     \FunctionTok{s}\NormalTok{(aveRooms) }\SpecialCharTok{+}
                     \FunctionTok{s}\NormalTok{(aveBedrooms) }\SpecialCharTok{+} 
                     \FunctionTok{s}\NormalTok{(population) }\SpecialCharTok{+} 
                     \FunctionTok{s}\NormalTok{(medianIncome) }\SpecialCharTok{+}
                     \FunctionTok{te}\NormalTok{(longitude, latitude) }\SpecialCharTok{+}
                     \FunctionTok{s}\NormalTok{(cityProximityScore) }\SpecialCharTok{+} 
                     \FunctionTok{s}\NormalTok{(bedroomsPerRoom) }\SpecialCharTok{+} 
                     \FunctionTok{s}\NormalTok{(incomePerRoom) }\SpecialCharTok{+}
                     \FunctionTok{s}\NormalTok{(distToSF) }\SpecialCharTok{+}
                     \FunctionTok{s}\NormalTok{(distToLA) }\SpecialCharTok{+}
                     \FunctionTok{s}\NormalTok{(cosDirToLA) }\SpecialCharTok{+}
                     \FunctionTok{s}\NormalTok{(cosDirToSF) }\SpecialCharTok{+}
                     \FunctionTok{s}\NormalTok{(sinDirToLA) }\SpecialCharTok{+}
                     \FunctionTok{s}\NormalTok{(sinDirToSF),  }
                   \AttributeTok{data =}\NormalTok{ train\_data\_imp,}
                   \AttributeTok{select =} \ConstantTok{TRUE}\NormalTok{) }\CommentTok{\# to reduce overfitting }


\CommentTok{\# Calculate MSE }
\NormalTok{test\_data\_imp }\OtherTok{=}\NormalTok{ data\_imp[}\SpecialCharTok{{-}}\NormalTok{train\_index, ]}
\NormalTok{gam\_pred\_2 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(gam\_model\_2, }\AttributeTok{newdata =}\NormalTok{ test\_data\_imp)}
\NormalTok{gam\_MSE\_2 }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((gam\_pred\_2 }\SpecialCharTok{{-}}\NormalTok{ test\_data\_imp}\SpecialCharTok{$}\NormalTok{medianHouseValue)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"The GAM MSE (improved) is"}\NormalTok{, gam\_MSE\_2, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## The GAM MSE (improved) is 3386501790
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#3386284239 }
\end{Highlighting}
\end{Shaded}

\subsection{2.1.3 Which model performs
better?}\label{which-model-performs-better}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# if any improvements to 2.1.2 occurs and changes the result between KNN and GAM, we need to update the following analysis.}
\end{Highlighting}
\end{Shaded}

In this regression problem, even after improvements, the KNN model
outperforms the GAM model as shown by its lower test MSE (approximately
3.00 billion versus 3.34 billion). This indicates that KNN better
captures the complex, nonlinear relationships in the data.

KNN does not appear to suffer from the curse of dimensionality in this
case, even with a relatively high number of predictors. After tuning the
number of neighbors, the model captures complex relationships in the
data without overfitting. Its strong test performance suggests that the
added predictors did not harm generalisation.

For the GAM model, attempts to incorporate additional variables
(e.g.~cityProximityScore) that potentially capture more relevant
information have resulted in only marginal improvements in MSE. This
suggests that the additive smooth terms in GAM are still too restrictive
and unable to fully model the underlying data complexity, limiting its
predictive power.

Overall, these results reflect the bias-variance trade-off: KNN achieves
lower bias through flexible local fitting and dimensionality reduction,
while GAM's higher bias---due to its smoother additive
assumptions---limits its performance despite generally lower variance.

\section{2.2 Classification models}\label{classification-models}

\subsection{2.2.1 New censoring boolean
column}\label{new-censoring-boolean-column}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add censoring column and drop id and medianHouseValue}
\NormalTok{data\_c }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{na.omit}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{censoring =} \FunctionTok{ifelse}\NormalTok{(medianHouseValue }\SpecialCharTok{==} \DecValTok{500001}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{id, }\SpecialCharTok{{-}}\NormalTok{medianHouseValue) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{oceanProximity =} \FunctionTok{as.factor}\NormalTok{(oceanProximity))}



\CommentTok{\# This dataframe does the same changes as above but is with our model improvements}
\NormalTok{data\_c\_imp }\OtherTok{\textless{}{-}}\NormalTok{ data\_imp }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{na.omit}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{censoring =} \FunctionTok{ifelse}\NormalTok{(medianHouseValue }\SpecialCharTok{==} \DecValTok{500001}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{id, }\SpecialCharTok{{-}}\NormalTok{medianHouseValue) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{oceanProximity =} \FunctionTok{as.factor}\NormalTok{(oceanProximity))}
\end{Highlighting}
\end{Shaded}

\subsection{2.2.2 Two classification
methods}\label{two-classification-methods}

\noindent \large \textbf{Logistic Regression:} \normalsize

\noindent In this case logistic regression can model the log-odds of
hitting the \(\$500,001\) cap (censoring \(= 1\)) as a linear function
of the predictors: \vspace{-1em}

\[\log \frac{Pr(\text{censor} = 1 | X_1,...,X_p)}{1 - Pr(\text{censor} = 1 | X_1,...,X_p)} = \beta_{0} + \beta_{1} \ \text{medianIncome} + \beta_{2} \ \text{housingMedianAge} + ...\]
This method is reasonable because:

\begin{enumerate}
  \item \textbf{Ideal for binary classification problems}: Designed specifically for binary response variables, logistic regression is suitable since in this case censoring take values of either $1$ (censored) or $0$ (not censored).
  \item \textbf{No strong distributional assumptions on }$\boldsymbol{X}$: Some of our variables may be skewed or heteroscedastic; logistic   regression simply cares that the logit is linear in $\boldsymbol{X}$ (where $\boldsymbol{X} = (X_1, ..., X_p)$).
  \item \textbf{Interpretable effects}: e.g. $e^{\beta_{1}}$ tells us how one unit ($\$1,000$) increases in medianIncome multiplies the odds of a      house price being censored.
  \item \textbf{Flexibility}: It estimates the probability that a given input belongs to a particular class, which also provides us with the flexibility of choosing the threshold for which optimises the testing correct rate with the assistance of the ROC curve.
\end{enumerate}

\noindent \large \textbf{KNN-CV:} \normalsize

\noindent In this case KNN-CV is a non-parametric approach that
classifies each block group by majority voting among its \(k\) closest
neighbours in the predictor space. Formally, for a test point \(x_0\),
let \(N_0\) be the indices of the \(k\) nearest training observations
(by Euclidean distance):

\[Pr(\text{censor} = 1|x_0) = \frac{1}{k}\sum_{i\in N_0}I(\text{censor}_i = 1)\]

\noindent and we predict the class with the larger estimated
probability. We choose \(k\) via cross-validation to pick the optimal
bias-variance trade-off. \textbackslash{}

\noindent This method is reasonable because:

\begin{enumerate}
  \item \textbf{Captures nonlinearity}: If censoring depends on complex interactions and requires a non-linear decision boundary, KNN can adapt without specifying a specific model form.
  \item \textbf{No parametric assumptions}: No distributional assumptions required--so it's robust to skewed, heteroskedastic variables.
  \item \textbf{Tunable bias-variance trade-off via cross-validation}: The ability to choose $k$ allows us to control the smoothness.
    \begin{itemize}
        \item Small $k \implies$ very flexible (low bias, high variance)
        \item Large $k \implies$ smoother (high bias, low variance)
        \item Using cross-validation empirically estimates the error for each $k$, letting us pick a $k$ value the minimises both over and under fitting
    \end{itemize}
\end{enumerate}

\subsection{2.2.3 Which classification method performs
better?}\label{which-classification-method-performs-better}

\textbf{How we have used the given data:}

\begin{itemize}
  \item We used a random 80/20 training and test split of the dataset to produce the test error rates.
  \item We dropped the \textit{id} and \textit{medianHouseValue} columns to ensure we only use legitimate predictors when training and testing our model.
  \item The logistic model thresholds at $0.5$, while KNN makes decisions based on the majority class among the nearest neighbors in the feature space.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Logistic Regression without Model Improvements}

\NormalTok{train\_data\_c }\OtherTok{\textless{}{-}}\NormalTok{ data\_c[train\_index, ]}
\NormalTok{test\_data\_c }\OtherTok{\textless{}{-}}\NormalTok{ data\_c[}\SpecialCharTok{{-}}\NormalTok{train\_index, ] }

\NormalTok{logit\_fit\_1 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(censoring }\SpecialCharTok{\textasciitilde{}}\NormalTok{ longitude }\SpecialCharTok{+} 
\NormalTok{                   latitude }\SpecialCharTok{+} 
\NormalTok{                   medianIncome }\SpecialCharTok{+} 
\NormalTok{                   housingMedianAge }\SpecialCharTok{+} 
\NormalTok{                   oceanProximity }\SpecialCharTok{+} 
\NormalTok{                   aveRooms }\SpecialCharTok{+} 
\NormalTok{                   aveBedrooms,}
                 \AttributeTok{data =}\NormalTok{ train\_data\_c, }
                 \AttributeTok{family =}\NormalTok{ binomial)}

\NormalTok{logit\_pred\_1 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{predict}\NormalTok{(logit\_fit\_1, }
                      \AttributeTok{newdata =}\NormalTok{ test\_data\_c, }
                      \AttributeTok{type =} \StringTok{"response"}\NormalTok{) }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{) }\CommentTok{\# use 0.5 for now }

\NormalTok{error\_rate }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(logit\_pred\_1 }\SpecialCharTok{!=}\NormalTok{ test\_data\_c}\SpecialCharTok{$}\NormalTok{censoring)}

\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Logistic Regression Prediction Error Rate:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(error\_rate, }\DecValTok{3}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Logistic Regression Prediction Error Rate: 0.031"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#library(caret)}
\CommentTok{\# Confusion matrix }
\NormalTok{confusion\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{confusionMatrix}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(logit\_pred\_1), }\FunctionTok{as.factor}\NormalTok{(test\_data\_c}\SpecialCharTok{$}\NormalTok{censoring))}
\FunctionTok{print}\NormalTok{(confusion\_matrix)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 3857  107
##          1   20  101
##                                          
##                Accuracy : 0.9689         
##                  95% CI : (0.9631, 0.974)
##     No Information Rate : 0.9491         
##     P-Value [Acc > NIR] : 4.094e-10      
##                                          
##                   Kappa : 0.599          
##                                          
##  Mcnemar's Test P-Value : 2.325e-14      
##                                          
##             Sensitivity : 0.9948         
##             Specificity : 0.4856         
##          Pos Pred Value : 0.9730         
##          Neg Pred Value : 0.8347         
##              Prevalence : 0.9491         
##          Detection Rate : 0.9442         
##    Detection Prevalence : 0.9704         
##       Balanced Accuracy : 0.7402         
##                                          
##        'Positive' Class : 0              
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ROC }
\CommentTok{\#library(caret)}
\CommentTok{\#library(pROC)}
\NormalTok{logit\_pred\_probs\_1 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(logit\_fit\_1, }\AttributeTok{newdata =}\NormalTok{ test\_data\_c, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{roc\_curve\_1 }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(test\_data\_c}\SpecialCharTok{$}\NormalTok{censoring, logit\_pred\_probs\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting levels: control = 0, case = 1
\end{verbatim}

\begin{verbatim}
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(roc\_curve\_1, }\AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{group_26_files/figure-latex/unnamed-chunk-10-1.png}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# error rate: 0.031}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Logistic Regression with Model Improvements}

\NormalTok{train\_data\_c\_imp }\OtherTok{\textless{}{-}}\NormalTok{ data\_c\_imp[train\_index, ]}
\NormalTok{test\_data\_c\_imp }\OtherTok{\textless{}{-}}\NormalTok{ data\_c\_imp[}\SpecialCharTok{{-}}\NormalTok{train\_index, ] }

\NormalTok{logit\_fit\_2 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(censoring }\SpecialCharTok{\textasciitilde{}}\NormalTok{ longitude }\SpecialCharTok{+} 
\NormalTok{                   latitude }\SpecialCharTok{+} 
\NormalTok{                   medianIncome }\SpecialCharTok{+} 
\NormalTok{                   housingMedianAge }\SpecialCharTok{+} 
\NormalTok{                   oceanProximity }\SpecialCharTok{+} 
\NormalTok{                   aveRooms }\SpecialCharTok{+} 
\NormalTok{                   aveBedrooms }\SpecialCharTok{+}
\NormalTok{                   cityProximityScore }\SpecialCharTok{+} 
\NormalTok{                   bedroomsPerRoom }\SpecialCharTok{+} 
\NormalTok{                   incomePerRoom }\SpecialCharTok{+}
\NormalTok{                   distToSF }\SpecialCharTok{+}
\NormalTok{                   distToLA }\SpecialCharTok{+}
\NormalTok{                   cosDirToLA }\SpecialCharTok{+}
\NormalTok{                   cosDirToSF }\SpecialCharTok{+}
\NormalTok{                   sinDirToLA }\SpecialCharTok{+}
\NormalTok{                   sinDirToSF,}
                 \AttributeTok{data =}\NormalTok{ train\_data\_c\_imp, }
                 \AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logit\_pred\_2 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{predict}\NormalTok{(logit\_fit\_2, }
                      \AttributeTok{newdata =}\NormalTok{ test\_data\_c\_imp, }
                      \AttributeTok{type =} \StringTok{"response"}\NormalTok{) }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{) }\CommentTok{\# use 0.5 for now }

\NormalTok{error\_rate }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(logit\_pred\_2 }\SpecialCharTok{!=}\NormalTok{ test\_data\_c\_imp}\SpecialCharTok{$}\NormalTok{censoring)}

\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"Logistic Regression Prediction Error Rate:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(error\_rate, }\DecValTok{3}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "Logistic Regression Prediction Error Rate: 0.029"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#library(caret)}
\CommentTok{\# Confusion matrix }
\NormalTok{confusion\_matrix }\OtherTok{\textless{}{-}} \FunctionTok{confusionMatrix}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(logit\_pred\_2), }\FunctionTok{as.factor}\NormalTok{(test\_data\_c\_imp}\SpecialCharTok{$}\NormalTok{censoring))}
\FunctionTok{print}\NormalTok{(confusion\_matrix)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 3854   95
##          1   23  113
##                                          
##                Accuracy : 0.9711         
##                  95% CI : (0.9655, 0.976)
##     No Information Rate : 0.9491         
##     P-Value [Acc > NIR] : 2.686e-12      
##                                          
##                   Kappa : 0.6426         
##                                          
##  Mcnemar's Test P-Value : 6.315e-11      
##                                          
##             Sensitivity : 0.9941         
##             Specificity : 0.5433         
##          Pos Pred Value : 0.9759         
##          Neg Pred Value : 0.8309         
##              Prevalence : 0.9491         
##          Detection Rate : 0.9435         
##    Detection Prevalence : 0.9667         
##       Balanced Accuracy : 0.7687         
##                                          
##        'Positive' Class : 0              
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# ROC }
\CommentTok{\#library(caret)}
\CommentTok{\#library(pROC)}
\NormalTok{logit\_pred\_probs\_2 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(logit\_fit\_2, }\AttributeTok{newdata =}\NormalTok{ test\_data\_c\_imp, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{roc\_curve\_2 }\OtherTok{\textless{}{-}} \FunctionTok{roc}\NormalTok{(test\_data\_c\_imp}\SpecialCharTok{$}\NormalTok{censoring, logit\_pred\_probs\_2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Setting levels: control = 0, case = 1
\end{verbatim}

\begin{verbatim}
## Setting direction: controls < cases
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(roc\_curve\_2, }\AttributeTok{main =} \StringTok{"ROC Curve"}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{group_26_files/figure-latex/unnamed-chunk-11-1.png}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# error rate: 0.029}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# KNN{-}CV Without Model Improvements}

\CommentTok{\# Prepare the predictors (X) and response (Y)}
\NormalTok{train.X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ longitude }\SpecialCharTok{+} 
\NormalTok{                          latitude }\SpecialCharTok{+} 
\NormalTok{                          housingMedianAge }\SpecialCharTok{+} 
\NormalTok{                          aveRooms }\SpecialCharTok{+} 
\NormalTok{                          aveBedrooms }\SpecialCharTok{+} 
\NormalTok{                          population }\SpecialCharTok{+} 
\NormalTok{                          medianIncome }\SpecialCharTok{+} 
\NormalTok{                          oceanProximity, }
                        \AttributeTok{data =}\NormalTok{ train\_data\_c)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{test.X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ longitude }\SpecialCharTok{+} 
\NormalTok{                         latitude }\SpecialCharTok{+} 
\NormalTok{                         housingMedianAge }\SpecialCharTok{+} 
\NormalTok{                         aveRooms }\SpecialCharTok{+} 
\NormalTok{                         aveBedrooms }\SpecialCharTok{+} 
\NormalTok{                         population }\SpecialCharTok{+} 
\NormalTok{                         medianIncome }\SpecialCharTok{+} 
\NormalTok{                         oceanProximity, }
                       \AttributeTok{data =}\NormalTok{ test\_data\_c)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{train.Y }\OtherTok{\textless{}{-}}\NormalTok{ train\_data\_c}\SpecialCharTok{$}\NormalTok{censoring}
\NormalTok{test.Y }\OtherTok{\textless{}{-}}\NormalTok{ test\_data\_c}\SpecialCharTok{$}\NormalTok{censoring}

\CommentTok{\# Use cross{-}validation on the training set to choose the best k}
\NormalTok{knn\_rp }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{15}\NormalTok{) \{}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\NormalTok{  knn.pred.cv }\OtherTok{\textless{}{-}} \FunctionTok{knn.cv}\NormalTok{(}\AttributeTok{train =}\NormalTok{ train.X, }\AttributeTok{cl =}\NormalTok{ train.Y, }\AttributeTok{k =}\NormalTok{ k)}
\NormalTok{  knn\_rp[k] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(knn.pred.cv }\SpecialCharTok{==}\NormalTok{ train.Y)}
\NormalTok{\}}

\NormalTok{k.cv }\OtherTok{\textless{}{-}} \FunctionTok{which.max}\NormalTok{(knn\_rp)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Best k selected by CV is:"}\NormalTok{, k.cv, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Best k selected by CV is: 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit and predict on the test set using the best k}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\NormalTok{knn\_pred\_c\_1 }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(}\AttributeTok{train =}\NormalTok{ train.X, }\AttributeTok{test =}\NormalTok{ test.X, }\AttributeTok{cl =}\NormalTok{ train.Y, }\AttributeTok{k =}\NormalTok{ k.cv)}

\CommentTok{\# Compute confusion matrix and error rate}
\FunctionTok{confusionMatrix}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(knn\_pred\_c\_1), }\FunctionTok{as.factor}\NormalTok{(test.Y))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 3876  199
##          1    1    9
##                                          
##                Accuracy : 0.951          
##                  95% CI : (0.944, 0.9575)
##     No Information Rate : 0.9491         
##     P-Value [Acc > NIR] : 0.2994         
##                                          
##                   Kappa : 0.0783         
##                                          
##  Mcnemar's Test P-Value : <2e-16         
##                                          
##             Sensitivity : 0.99974        
##             Specificity : 0.04327        
##          Pos Pred Value : 0.95117        
##          Neg Pred Value : 0.90000        
##              Prevalence : 0.94908        
##          Detection Rate : 0.94884        
##    Detection Prevalence : 0.99755        
##       Balanced Accuracy : 0.52151        
##                                          
##        'Positive' Class : 0              
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\AttributeTok{knn\_err\_c\_1 =} \FunctionTok{mean}\NormalTok{(knn\_pred\_c\_1 }\SpecialCharTok{!=}\NormalTok{ test.Y))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.04895961
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"The KNN error rate is"}\NormalTok{, }\FunctionTok{round}\NormalTok{(knn\_err\_c\_1, }\DecValTok{4}\NormalTok{), }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## The KNN error rate is 0.049
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#The KNN error rate is 0.049 }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# KNN{-}CV with Model Improvements}

\CommentTok{\# Prepare the predictors (X) and response (Y)}
\NormalTok{train.X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ longitude }\SpecialCharTok{+} 
\NormalTok{                          latitude }\SpecialCharTok{+} 
\NormalTok{                          medianIncome }\SpecialCharTok{+} 
\NormalTok{                          housingMedianAge }\SpecialCharTok{+} 
\NormalTok{                          oceanProximity }\SpecialCharTok{+} 
\NormalTok{                          aveRooms }\SpecialCharTok{+} 
\NormalTok{                          aveBedrooms }\SpecialCharTok{+}
\NormalTok{                          cityProximityScore }\SpecialCharTok{+} 
\NormalTok{                          bedroomsPerRoom }\SpecialCharTok{+} 
\NormalTok{                          incomePerRoom }\SpecialCharTok{+}
\NormalTok{                          distToSF }\SpecialCharTok{+}
\NormalTok{                          distToLA }\SpecialCharTok{+}
\NormalTok{                          cosDirToLA }\SpecialCharTok{+}
\NormalTok{                          cosDirToSF }\SpecialCharTok{+}
\NormalTok{                          sinDirToLA }\SpecialCharTok{+}
\NormalTok{                          sinDirToSF, }
                        \AttributeTok{data =}\NormalTok{ train\_data\_c\_imp)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{test.X }\OtherTok{\textless{}{-}} \FunctionTok{model.matrix}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{  longitude }\SpecialCharTok{+} 
\NormalTok{                          latitude }\SpecialCharTok{+} 
\NormalTok{                          medianIncome }\SpecialCharTok{+} 
\NormalTok{                          housingMedianAge }\SpecialCharTok{+} 
\NormalTok{                          oceanProximity }\SpecialCharTok{+} 
\NormalTok{                          aveRooms }\SpecialCharTok{+} 
\NormalTok{                          aveBedrooms }\SpecialCharTok{+}
\NormalTok{                          cityProximityScore }\SpecialCharTok{+} 
\NormalTok{                          bedroomsPerRoom }\SpecialCharTok{+} 
\NormalTok{                          incomePerRoom }\SpecialCharTok{+}
\NormalTok{                          distToSF }\SpecialCharTok{+}
\NormalTok{                          distToLA }\SpecialCharTok{+}
\NormalTok{                          cosDirToLA }\SpecialCharTok{+}
\NormalTok{                          cosDirToSF }\SpecialCharTok{+}
\NormalTok{                          sinDirToLA }\SpecialCharTok{+}
\NormalTok{                          sinDirToSF, }
                       \AttributeTok{data =}\NormalTok{ test\_data\_c\_imp)[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{train.Y }\OtherTok{\textless{}{-}}\NormalTok{ train\_data\_c\_imp}\SpecialCharTok{$}\NormalTok{censoring}
\NormalTok{test.Y }\OtherTok{\textless{}{-}}\NormalTok{ test\_data\_c\_imp}\SpecialCharTok{$}\NormalTok{censoring}

\CommentTok{\# Use cross{-}validation on the training set to choose the best k}
\NormalTok{knn\_rp }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (k }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{15}\NormalTok{) \{}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\NormalTok{  knn.pred.cv }\OtherTok{\textless{}{-}} \FunctionTok{knn.cv}\NormalTok{(}\AttributeTok{train =}\NormalTok{ train.X, }\AttributeTok{cl =}\NormalTok{ train.Y, }\AttributeTok{k =}\NormalTok{ k)}
\NormalTok{  knn\_rp[k] }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(knn.pred.cv }\SpecialCharTok{==}\NormalTok{ train.Y)}
\NormalTok{\}}

\NormalTok{k.cv }\OtherTok{\textless{}{-}} \FunctionTok{which.max}\NormalTok{(knn\_rp)}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Best k selected by CV is:"}\NormalTok{, k.cv, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Best k selected by CV is: 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Fit and predict on the test set using the best k}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{5}\NormalTok{)}
\NormalTok{knn\_pred\_c\_2 }\OtherTok{\textless{}{-}} \FunctionTok{knn}\NormalTok{(}\AttributeTok{train =}\NormalTok{ train.X, }\AttributeTok{test =}\NormalTok{ test.X, }\AttributeTok{cl =}\NormalTok{ train.Y, }\AttributeTok{k =}\NormalTok{ k.cv)}

\CommentTok{\# Compute confusion matrix and error rate}
\FunctionTok{confusionMatrix}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(knn\_pred\_c\_2), }\FunctionTok{as.factor}\NormalTok{(test.Y))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 3859  107
##          1   18  101
##                                           
##                Accuracy : 0.9694          
##                  95% CI : (0.9636, 0.9745)
##     No Information Rate : 0.9491          
##     P-Value [Acc > NIR] : 1.419e-10       
##                                           
##                   Kappa : 0.603           
##                                           
##  Mcnemar's Test P-Value : 3.519e-15       
##                                           
##             Sensitivity : 0.9954          
##             Specificity : 0.4856          
##          Pos Pred Value : 0.9730          
##          Neg Pred Value : 0.8487          
##              Prevalence : 0.9491          
##          Detection Rate : 0.9447          
##    Detection Prevalence : 0.9709          
##       Balanced Accuracy : 0.7405          
##                                           
##        'Positive' Class : 0               
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\AttributeTok{knn\_err\_c\_2 =} \FunctionTok{mean}\NormalTok{(knn\_pred\_c\_2 }\SpecialCharTok{!=}\NormalTok{ test.Y))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.03059976
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cat}\NormalTok{(}\StringTok{"The KNN error rate is"}\NormalTok{, }\FunctionTok{round}\NormalTok{(knn\_err\_c\_2, }\DecValTok{4}\NormalTok{), }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## The KNN error rate is 0.0306
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#The KNN error rate is 0.0306 }
\end{Highlighting}
\end{Shaded}

\subsection{2.3.4 Justifying the better classification
method}\label{justifying-the-better-classification-method}

Need to update

In our 80/20 hold-out test we observed:

\begin{table}[ht]
\centering
\begin{tabular}{l c}
\hline
\textbf{Model}            & \textbf{Misclassification Rate} \\ 
\hline
Logistic Regression       & 0.0310                          \\
Linear Discriminant Analysis (LDA) & 0.0343                          \\
\hline
\end{tabular}
\caption{Test-set misclassification rates for Logistic Regression and LDA.}
\label{tab:misclass}
\end{table}

Therefore, by this test Logistic Regression performs better than LDA,
since it has the lower test error rate.

\textbf{Reasoning:}

\begin{enumerate}
  \item Bias
  \begin{itemize}
    \item Logistic regression only makes one structural assumption: that the log-odds of censoring is a linear function of the predictors. Also there is only 
  \end{itemize}
  \item Variance
  \begin{itemize}
    \item
  \end{itemize}
  \item Trade-off
  \begin{itemize}
    \item
  \end{itemize}
\end{enumerate}

Note: We did not generate ROC curves for KNN because it does not provide
continuous probability scores but rather hard class labels. ROC analysis
requires varying the classification threshold across a range of values,
which is not possible with the standard KNN output. While probabilistic
adaptations of KNN exist, they were not necessary here since our
objective was classification accuracy, not probability estimation.

\subsection{2.2.4 Justifying the better classification
method}\label{justifying-the-better-classification-method-1}

Logistic regression outperformed KNN-CV in our 80/20 hold-out test, with
test errors of 0.0290 and 0.0306 respectively. Logistic regression is a
parametric model, it assumes a specific relationship between predictors
and the target, resulting in higher bias but lower variance. This
rigidity works in its favour when the underlying structure of the data
is close to linear, as it avoids fitting noise and reduces the risk of
overfitting. KNN, on the other hand, is non-parametric and makes very
few assumptions about the data's form. It is flexible and can capture
complex, non-linear patterns, but that flexibility comes at the cost of
high variance, especially in high-dimensional settings.

While KNN's flexibility can be useful, it becomes increasingly unstable
in high-dimensional spaces where data points are more sparsely
distributed and neighbourhoods lose their meaning. Specifically, the
distance between the nearest and farthest neighbours shrinks, making it
hard to define what counts as ``close,'' which undermines the core logic
of KNN. Even with cross-validation to choose the optimal k, KNN was
still overfitting, likely due to this dimensionality issue. Logistic
regression, despite its linear constraint, handled the complexity of the
data more gracefully by not reacting too much to the noise.

The lower test error achieved by logistic regression shows that its
higher bias was actually beneficial here, it provided a smoother, more
stable model that generalised well. So while KNN theoretically offers
more flexibility, in this case it became a liability, and logistic
regression ultimately struck the better balance. To note, the test error
is only marginally larger for the KNN, and both are relatively low so it
is clear that both models are effective, just for this random seed
logistic regression is the winner.

\section{2.3 A hybrid approach}\label{a-hybrid-approach}

\subsection{2.3.1 Dicussing feasibility of the
approach}\label{dicussing-feasibility-of-the-approach}

Estimate using improved KNN and then apply the censoring from logistic.
More explanation necessary

To address the censoring issue in the dataset, a hybrid approach that
combines regression and classification models is both feasible and
potentially very effective. The best regression model from 2.1.3 is used
to predict house values, while the best classifier from 2.2.2 identifies
whether a given observation is censored; i.e.~whether the true house
value exceeds the \$500,001 threshold. This setup allows us to adjust
regression predictions when the classifier indicates censoring, helping
to reduce the bias that occurs when censored values are treated as if
they were exact rather than right censored.

The procedure involves four main steps. First, we use the KNN regression
model to predict house prices. Second, a logistic regression classifier
estimates the probability of censoring. Third, we adjust the regression
predictions by setting them to 500001 for observations predicted to be
censored. Finally, we evaluate the performance of this adjusted model
using test MSE. This combined method leverages the strengths of both
models and is a practical way to improve prediction accuracy in the
presence of censoring.

\subsection{2.3.2 test MSE of medianHousingValue using the
approach}\label{test-mse-of-medianhousingvalue-using-the-approach}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Step 1: Predict house prices with KNN regression}
\NormalTok{knn\_pred\_2 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(knn\_reg\_model\_2, }\AttributeTok{newdata =}\NormalTok{ test\_data\_encoded)}

\CommentTok{\# Step 2: Predict censoring with logistic regression}
\NormalTok{logit\_pred\_2 }\OtherTok{\textless{}{-}} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{predict}\NormalTok{(logit\_fit\_2, }\AttributeTok{newdata =}\NormalTok{ test\_data\_c\_imp, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{) }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{)}

\CommentTok{\# Step 3: Adjust predicted house prices}
\CommentTok{\# If censored predicted, clip to 500001}
\NormalTok{adjusted\_preds }\OtherTok{\textless{}{-}} \FunctionTok{ifelse}\NormalTok{(logit\_pred\_2 }\SpecialCharTok{==} \DecValTok{1}\NormalTok{, }\DecValTok{500001}\NormalTok{, knn\_pred\_2)}

\CommentTok{\# Step 4: Compute MSE using all observations}
\NormalTok{final\_mse }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((adjusted\_preds }\SpecialCharTok{{-}}\NormalTok{ test\_data\_encoded}\SpecialCharTok{$}\NormalTok{medianHouseValue)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}

\FunctionTok{cat}\NormalTok{(}\StringTok{"Final Test MSE:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(final\_mse, }\DecValTok{2}\NormalTok{), }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Final Test MSE: 2997065945
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Final Test MSE: 3025628814 }
\CommentTok{\# this is worse then KNN\_2}
\end{Highlighting}
\end{Shaded}

\subsection{2.3.3 Comparison of the accuracy of this procedure to model
in
2.1.3}\label{comparison-of-the-accuracy-of-this-procedure-to-model-in-2.1.3}

No test MSE for combined model is worse.

Other note, random test/train sample too.

The combined model clips predictions to 500001 when the logistic model
predicts censoring, but if that prediction is wrong, it forces the value
far from the true (uncensored) price, increasing error. KNN, on the
other hand, always predicts a value --- even for censored cases --- and
sometimes lands closer to the unknown true price (which is at least
500001), purely by chance. This makes KNN appear to perform better in
terms of MSE, even though it doesn't account for censoring and might be
underestimating the true values.

The hybrid model resulted in a slightly higher test MSE (3,025,628,814)
compared to the original KNN regression model from 2.1.3
(2,999,761,722). These results were obtained using a random 80/20
train-test split of the data the same split used throughout the course
of this report. Given how close the two MSE values are, it's possible
that on a different random sample, the hybrid model could perform
slightly better. Overall, the difference is minimal, suggesting that
both models offer comparable predictive performance in this context.

The reason the hybrid model does not consistently outperform KNN likely
stems from how it handles censoring using a hard classification
decision. Specifically, it clips predicted values to 500001 whenever the
logistic classifier predicts an observation is censored. This is because
logistic regression applies a strict decision boundary (at 0.5) to make
this call, which introduces a sharp threshold with no gradual
transition. This can be problematic near the boundary, where small
changes in input may flip the classification, yet the model treats the
output as definitively censored or not. If the classifier incorrectly
predicts censoring, the hard cap forces the prediction to 500001, which
can be significantly off from the true (uncensored) value. In contrast,
the KNN model always outputs a continuous estimate. When the classifier
misclassifies an uncensored point as censored, the true value is often
still closer to the KNN prediction than to the fixed 500001. As a
result, KNN may show better MSE performance, despite not explicitly
addressing censoring.

\subsubsection{A probability-weighted
approach}\label{a-probability-weighted-approach}

The idea behind the probability-weighted approach is to treat the final
house price prediction as the expected value under uncertainty about
whether a value is censored.

We don't know for sure whether an observation is censored, but we have a
probability estimate from the logistic model. So, instead of making a
hard decision, we compute the weighted average of:

the threshold value (\(500{,}001\)) if it is censored, and the KNN
regression prediction if it is not censored, weighted by the probability
of each case.

Let \(p_i = \mathbb{P}(\text{censored} \mid \text{features}_i)\), and
let \(\hat{y}_i^{\text{KNN}}\) be the KNN regression prediction. The
probability-weighted hybrid prediction is then:

\[
\hat{y}^*_i = p_i \cdot 500001 + (1 - p_i) \cdot \hat{y}_i^{\text{KNN}}
\]

This is interpreted as the expected value of the house price under
uncertainty about censoring.

This adjusted hybrid prediction approach is highly feasible, as it
simply disregard the 0.5 threshold in the hard-decision classification
previously, and simply use the probability output from logistic
regression in the medianHousingValue prediction. This eliminates the
difficulty about choosing an appropriate threshold, as well as avoid
omission of useful information due to a fixed, arbitrary cutoff
(e.g.~~0.5).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Some adjustments to the hard decision classification? }

\CommentTok{\# Step 1: Predict house prices with KNN regression (same as before)}
\NormalTok{knn\_pred\_2 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(knn\_reg\_model\_2, }\AttributeTok{newdata =}\NormalTok{ test\_data\_encoded)}

\CommentTok{\# Step2: Predict censoring with logistic regression}
 \CommentTok{\# Instead of the hard classification decision, use }
\NormalTok{logit\_prob\_2 }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(logit\_fit\_2, }\AttributeTok{newdata =}\NormalTok{ test\_data\_c\_imp, }\AttributeTok{type =} \StringTok{"response"}\NormalTok{)}

\CommentTok{\# Step 3: Adjust predicted house prices}
\NormalTok{adjusted\_preds\_ver2 }\OtherTok{\textless{}{-}}\NormalTok{ logit\_prob\_2 }\SpecialCharTok{*} \DecValTok{500001} \SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ logit\_prob\_2) }\SpecialCharTok{*}\NormalTok{ knn\_pred\_2}
 \CommentTok{\# this is like a expected value }

\CommentTok{\# Step4: }
\NormalTok{final\_mse\_ver2 }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{((adjusted\_preds\_ver2 }\SpecialCharTok{{-}}\NormalTok{ test\_data\_encoded}\SpecialCharTok{$}\NormalTok{medianHouseValue)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}

\NormalTok{(final\_mse\_ver2) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2975741318
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# final\_mse\_ver2 is 2975741318}

\FunctionTok{cat}\NormalTok{(}\StringTok{"Final Test MSE using probability{-}weighted prediction:"}\NormalTok{, }\FunctionTok{round}\NormalTok{(final\_mse\_ver2, }\DecValTok{2}\NormalTok{), }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Final Test MSE using probability-weighted prediction: 2975741318
\end{verbatim}

The modified hybrid model has a slightly lower MSE (2,975,741,318)
compared to the KNN regression model from 2.1.3 (2,999,761,722). This
improvement can be attributed to how the probability-weighted method
handles uncertainty near the censoring boundary. Instead of applying a
hard classification rule (i.e., clipping to 500,001 when the probability
of censoring exceeds 0.5), this method uses a soft probabilistic
adjustment. This approach acts like an expected value under uncertainty,
allowing smoother transitions near the threshold.

However, this 0.8\% decrease is not significant. It may fall within the
range of random variation due to the specific train-test split or model
variability. While the probabilistic adjustment offers a more refined
treatment of censoring, the overall improvement is marginal, suggesting
that both approaches perform similarly in practice.

\end{document}
