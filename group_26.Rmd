---
title: "Group-26 AAD-Assignment-2"
author: "Omar, Eloise, Alina, Sue"
date: '`r Sys.Date()`'
output:
  pdf_document:
    toc: true
    toc_depth: 4
    number_sections: false
    fig_width: 8
    fig_height: 6
    keep_tex: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      dev = "png",
                      dpi = 150)

# Load relevant libraries
library(dplyr)
library(ggplot2)
library(gam) # for fitting gam models
library(caret)
```

# 2.1 Nonlinear model vs non-parametric model

\noindent \large \textbf{GAM:} \normalsize

\noindent \textbf{Pros}
\begin{itemize}
  \item \textbf{Flexibility to capture non-linearity}. In a GAMM we replace each linear term $\beta_{k} x_{k}$ with a smooth function $f_{k}(x_{k})$,     meaning we can automatically model non-linear relationships between each predictor and the response without having to manually try out different      transformation on each variable individually.
  \item \textbf{Non-linear fits may be more accurate}. The ability of creating this non-linear predictor and response relationship may make our model     more accurate when predicting the medianHousingValue.
  \item \textbf{Interperability from additivity}. Since a GAMM is additive, we can examine the effect of each $X_j$ on $Y$ individually while holding      all of the other variables fixed. Therefore, we can understand each variables individual effect on house value.
  \item \textbf{Control over smoothness}. Each $f_j$ comes with an associated smoothing parameter (or degrees of freedom), making it straightforward      to trade bias and variance  (e.g. via cross-validation).
\end{itemize}

\noindent \textbf{Cons}
\begin{itemize}
  \item \textbf{Additivity assumption restriction}. If there are strong synergistic effects like between location (longitude/latitude) and income         \textendash an additive model will not capture them unless explicit interaction terms $X_j \times X_k$ are included or low-dimensional               interactions function of the form $f_{jk}(X_j, X_k)$ are manually introduced. 
  \item \textbf{Computational cost}. Fitting this model to over \textasciitilde$20,600$ observations and multiple smoothers can be very slow when        compared to fitting a single parametric method.
\end{itemize}

\noindent \large \textbf{KNN:} \normalsize

\noindent \textbf{Pros}
\begin{itemize}
  \item \textbf{Completely non-parametric}. KNN makes no assumptions about the form of $f(X)$, allowing the model to potentially fit better than a      parametric model. At a point $x_0$ KNN averages the responses of the $K$ closest training blocks: $\hat{f}(x_{0}) = \frac{1}{K} \sum_{x_{i}           \in N_{0}}{y_i}$, where $N_0$ is the set of the $K$ nearest neighbors.
  \item \textbf{Control of bias-variance}. A small $K$ yields a very flexible, low-bias but high-variance fit; large $K$ yields a smoother,      lower-variance fit.
\end{itemize}

\noindent \textbf{Cons}
\begin{itemize}
  \item \textbf{Dimensoniality constraints}. As the number of predictors grows, the "nearest" neighbors tend to be far away in a high-dimensional       space, so KNN's performance degrades rapidly as the number of predictors grow.
  \item \textbf{Distance metric sensitivity}. With a KNN you must scale the numeric features and encode the categorical features (e.g.                  oceanProximity) carefully. Otherwise poorly scaled or encoded features can dominate the distance of the nearest neighbors calculation.
  \item \textbf{Computationally intensive with predictions}. For each new group of predictions all \textasciitilde$20,600$ must be computed to find     the $K$ nearest, which can be very intensive and slow.
  \item \textbf{Low interperability}. There is no simple way to explain a KNN prediction beyond pointing to the raw neighbors and their average.
\end{itemize}

\noindent \large \textbf{In this housing-price context: } \normalsize
\begin{itemize}
  \item \textbf{GAMM:} it is likely to give an interpretable model with, as we can analyse the partial-effects (e.g. how median income or ocean         proximity individually affects price), and we can capture smooth non-linear trends.
  \item \textbf{KNN:} can capture complex interactions, between predictors, automatically, but with eight predictors (including a categorical     one)   it may run into high-dimensionality issues, making distance-based averaging unstable and slow.
\end{itemize} 


## 2.1.1 GAM vs KNN approach

```{r}
# Load data and clean NA
data <- read.csv("Assignt2_data.csv")
data <- na.omit(data)

# Split data into training (70%) and test (30%) sets
set.seed(26)
n <- nrow(data)
train_index <- sample(1:n, size=0.7*n)
data_train <- data[train_index, ]
data_test <- data[-train_index, ]

# Fit GAM model using smoothing spline (df=4 for now) on each continuous predictor
gam_fit <- gam(
  medianHouseValue ~ 
    s(longitude, df = 4) +
    s(latitude, df = 4) +
    s(housingMedianAge, df = 4) +
    s(aveRooms, df = 4) +
    s(aveBedrooms, df = 4) +
    s(population, df = 4) +
    s(medianIncome, df = 4),
  data=data_train
)
#par(mfrow = c(1, 4))
#plot.Gam(gam_fit, se=TRUE, col="blue")

preds_gam <- predict(gam_fit, newdata = data_test)
gam_mse <- mean((data_test$medianHouseValue - preds_gam)^2)
gam_mse

# Fit KNN model

```


## 2.1.2 GAM vs KNN Regression Model

## 2.1.3 Which model performs better?

# 2.2 Classification models

## 2.2.1 Two classification methods

## 2.2.2 Suggested classifiers

## 2.2.3 Which classification method performs better?

# 2.3 A hybrid approach

## 2.3.1 Dicussing feasibility of the approach

## 2.3.2 test MSE of medianHousingValue using the approach

## 2.3.3 Comparison of the accuracy of this procedure to model in 2.1.3


