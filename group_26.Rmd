---
title: "Group-26 AAD-Assignment-2"
author: "Omar, Eloise, Alina, Sue"

date: "`r Sys.Date()`"
  html_document:
    toc: true
    toc_depth: '4'
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      dev = "png",
                      dpi = 150)

# Load relevant libraries
library(dplyr)
library(ggplot2)
#library(gam)
library(caret)
library(tidyverse)
library(mgcv) # for GAM
library(MASS) # for LDA 
library(class) # for KNN
library(geosphere)
library(pROC)
```

# 2.1 Nonlinear model vs non-parametric model



## 2.1.1 GAM vs KNN approach


\noindent \large \textbf{GAM:} \normalsize

\noindent \textbf{Pros}
\begin{itemize}
  \item \textbf{Flexibility to capture non-linearity}. In a GAMM we replace each linear term $\beta_{k} x_{k}$ with a smooth function $f_{k}(x_{k})$,     meaning we can automatically model non-linear relationships between each predictor and the response without having to manually try out different      transformation on each variable individually.
  \item \textbf{Non-linear fits may be more accurate}. The ability of creating this non-linear predictor and response relationship may make our model     more accurate when predicting the medianHousingValue.
  \item \textbf{Interperability from additivity}. Since a GAMM is additive, we can examine the effect of each $X_j$ on $Y$ individually while holding      all of the other variables fixed. Therefore, we can understand each variables individual effect on house value.
  \item \textbf{Control over smoothness}. Each $f_j$ comes with an associated smoothing parameter (or degrees of freedom), making it straightforward      to trade bias and variance  (e.g. via cross-validation).
\end{itemize}

\noindent \textbf{Cons}
\begin{itemize}
  \item \textbf{Additivity assumption restriction}. If there are strong synergistic effects like between location (longitude/latitude) and income         \textendash an additive model will not capture them unless explicit interaction terms $X_j \times X_k$ are included or low-dimensional               interactions function of the form $f_{jk}(X_j, X_k)$ are manually introduced. 
  \item \textbf{Computational cost}. Fitting this model to over \textasciitilde$20,600$ observations and multiple smoothers can be very slow when        compared to fitting a single parametric method.
\end{itemize}

\noindent \large \textbf{KNN:} \normalsize

\noindent \textbf{Pros}
\begin{itemize}
  \item \textbf{Completely non-parametric}. KNN makes no assumptions about the form of $f(X)$, allowing the model to potentially fit better than a      parametric model. At a point $x_0$ KNN averages the responses of the $K$ closest training blocks: $\hat{f}(x_{0}) = \frac{1}{K} \sum_{x_{i}           \in N_{0}}{y_i}$, where $N_0$ is the set of the $K$ nearest neighbors.
  \item \textbf{Control of bias-variance}. A small $K$ yields a very flexible, low-bias but high-variance fit; large $K$ yields a smoother,      lower-variance fit.
\end{itemize}

\noindent \textbf{Cons}
\begin{itemize}
  \item \textbf{Dimensoniality constraints}. As the number of predictors grows, the "nearest" neighbors tend to be far away in a high-dimensional       space, so KNN's performance degrades rapidly as the number of predictors grow.
  \item \textbf{Distance metric sensitivity}. With a KNN you must scale the numeric features and encode the categorical features (e.g.                  oceanProximity) carefully. Otherwise poorly scaled or encoded features can dominate the distance of the nearest neighbors calculation.
  \item \textbf{Computationally intensive with predictions}. For each new group of predictions all \textasciitilde$20,600$ must be computed to find     the $K$ nearest, which can be very intensive and slow.
  \item \textbf{Low interperability}. There is no simple way to explain a KNN prediction beyond pointing to the raw neighbors and their average.
\end{itemize}

\noindent \large \textbf{In this housing-price context: } \normalsize
\begin{itemize}
  \item \textbf{GAMM:} it is likely to give an interpretable model with, as we can analyse the partial-effects (e.g. how median income or ocean         proximity individually affects price), and we can capture smooth non-linear trends.
  \item \textbf{KNN:} can capture complex interactions, between predictors, automatically, but with eight predictors (including a categorical     one)   it may run into high-dimensionality issues, making distance-based averaging unstable and slow.
\end{itemize} 





## 2.1.2 GAM vs KNN Regression Model



We will use the cleaned data (omitting all data point that contains "NA") instead of the original raw dataset. We split the data into a training set (80% of the original data) and a test set. 

```{r}
# load the data 
data <- read.csv("Assignt2_data.csv")
data <- na.omit(data)
# View(data)
```


```{r}
# separate the train and test set out 
set.seed(1)
train_index <- createDataPartition(data$medianHouseValue, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data  <- data[-train_index, ]
```


```{r}
# Basic KNN
knn_reg_model_1 <- train(
  medianHouseValue ~ ., 
  data = train_data,
  method = "knn",
  preProcess = c("center", "scale"),
  tuneGrid = data.frame(k = 1:10),  # test k = 1 to 10
  trControl = trainControl(method = "cv", number = 5) 
) 

# best k 
print(knn_reg_model_1)

# KNN MSE calculation 
knn_pred_1 <- predict(knn_reg_model_1, newdata = test_data)
knn_MSE_1 <- mean((knn_pred_1 - test_data$medianHouseValue)^2) 
cat("The KNN MSE is", knn_MSE_1, "\n")

```
### Model Improvements

```{r}
# Using the predictors from Asmt1 ? 
data_imp <- read.csv("Assignt2_data.csv")
data_imp <- na.omit(data_imp)

# bedroomsPerRoom
data_imp$bedroomsPerRoom <- data_imp$aveBedrooms / data_imp$aveRooms


# incomePerRoom
data_imp$incomePerRoom = data_imp$medianIncome / data_imp$aveRooms

# Compute distances
la_coords <- c(-118.24, 34.05)   # (longitude, latitude)
sf_coords <- c(-122.42, 37.77)

data_imp$distToLA <- apply(data_imp[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, la_coords) / 1000
})
data_imp$distToSF <- apply(data_imp[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, sf_coords) / 1000
})

# Compute directions
data_imp$dirToLA <- atan2(data_imp$latitude - la_coords[2], data_imp$longitude - la_coords[1])
data_imp$dirToSF <- atan2(data_imp$latitude - sf_coords[2], data_imp$longitude - sf_coords[1])

# Cosine and sine of direction
data_imp$cosDirToLA <- cos(data_imp$dirToLA)
data_imp$sinDirToLA <- sin(data_imp$dirToLA)
data_imp$cosDirToSF <- cos(data_imp$dirToSF)
data_imp$sinDirToSF <- sin(data_imp$dirToSF)
data_imp$dirToLA <- NULL
data_imp$dirToSF <- NULL

# Composite proximity score
data_imp$cityProximityScore <- 1 / (1 + data_imp$distToLA) + 1 / (1 + data_imp$distToSF)

# Add these to test and train
set.seed(1)
train_data_imp <- data_imp[train_index, ]
test_data_imp  <- data_imp[-train_index, ]




```



```{r}
# with model improvements
# Hot encoding the variables

# Convert oceanProximity to factor (if not already)
train_data_imp$oceanProximity <- as.factor(train_data_imp$oceanProximity)
test_data_imp$oceanProximity <- as.factor(test_data_imp$oceanProximity)

# One-hot encode using dummyVars
dummies <- dummyVars(~ ., data = train_data_imp)
train_data_encoded <- predict(dummies, newdata = train_data_imp)
test_data_encoded <- predict(dummies, newdata = test_data_imp)

# Convert to data frame
train_data_encoded <- as.data.frame(train_data_encoded)
test_data_encoded <- as.data.frame(test_data_encoded)



knn_reg_model_2 <- train(
  medianHouseValue ~ ., 
  data = train_data_encoded,
  method = "knn",
  preProcess = c("center", "scale"),
  tuneGrid = data.frame(k = 1:15),  # test k = 1 to 15
  trControl = trainControl(method = "cv", number = 5) 
) 

# best k 
print(knn_reg_model_2)

# KNN MSE calculation 
knn_pred_2 <- predict(knn_reg_model_2, newdata = test_data_encoded)
knn_MSE_2 <- mean((knn_pred_2 - test_data_encoded$medianHouseValue)^2) 
cat("The KNN MSE (improved) is", knn_MSE_2, "\n")


#The KNN MSE is 3697939911 with out encoding
#With encoding  3694531899
#PCA makes it worse in general
#improve when tested more k 3650562433
# best now is: 2999761722 
```


```{r}
# GAM 
# the split is the same as before 

# train the GAM 
gam_model_1 <- mgcv::gam(medianHouseValue ~ s(latitude) + s(longitude) + 
                   s(housingMedianAge) + s(aveRooms) +
                   s(aveBedrooms) + s(population) +
                   s(medianIncome) + factor(oceanProximity), data = train_data)

# Calculate MSE 
gam_pred_1 <- predict(gam_model_1, newdata = test_data)
gam_MSE_1 <- mean((gam_pred_1 - test_data$medianHouseValue)^2)
cat("The GAM MSE is", gam_MSE_1, "\n")

#4258008383 
```

###### some justifications required: 

```{r}
# improvements
train_data_imp = data_imp[train_index, ]
gam_model_2 <- mgcv::gam(medianHouseValue ~ 
                     s(housingMedianAge) + 
                     s(aveRooms) +
                     s(aveBedrooms) + 
                     s(population) + 
                     s(medianIncome) +
                     te(longitude, latitude) +
                     s(cityProximityScore) + 
                     s(bedroomsPerRoom) + 
                     s(incomePerRoom) +
                     s(distToSF) +
                     s(distToLA) +
                     s(cosDirToLA) +
                     s(cosDirToSF) +
                     s(sinDirToLA) +
                     s(sinDirToSF),  
                   data = train_data_imp,
                   select = TRUE) # to reduce overfitting 


# Calculate MSE 
test_data_imp = data_imp[-train_index, ]
gam_pred_2 <- predict(gam_model_2, newdata = test_data_imp)
gam_MSE_2 <- mean((gam_pred_2 - test_data_imp$medianHouseValue)^2)
cat("The GAM MSE (improved) is", gam_MSE_2, "\n")

#3386284239 
```


## 2.1.3 Which model performs better?

```{r}
# if any improvements to 2.1.2 occurs and changes the result between KNN and GAM, we need to update the following analysis.
```

In this regression problem, even after improvements, the KNN model outperforms the GAM model as shown by its lower test MSE (approximately 3.00 billion versus 3.34 billion). This indicates that KNN better captures the complex, nonlinear relationships in the data.

KNN does not appear to suffer from the curse of dimensionality in this case, even with a relatively high number of predictors. After tuning the number of neighbors, the model captures complex relationships in the data without overfitting. Its strong test performance suggests that the added predictors did not harm generalisation.

For the GAM model, attempts to incorporate additional variables (e.g. cityProximityScore) that potentially capture more relevant information have resulted in only marginal improvements in MSE. This suggests that the additive smooth terms in GAM are still too restrictive and unable to fully model the underlying data complexity, limiting its predictive power.

Overall, these results reflect the bias-variance trade-off: KNN achieves lower bias through flexible local fitting and dimensionality reduction, while GAM’s higher bias—due to its smoother additive assumptions—limits its performance despite generally lower variance.



# 2.2 Classification models

## 2.2.1 New censoring boolean column
```{r}


# Add censoring column and drop id and medianHouseValue
data_c <- data %>%
  na.omit() %>%
  mutate(censoring = ifelse(medianHouseValue == 500001, 1, 0)) %>%
  dplyr::select(-id, -medianHouseValue) %>%
  mutate(oceanProximity = as.factor(oceanProximity))


# This dataframe does the same changes as above but is with our model improvements
data_c_imp <- data_imp %>%
  na.omit() %>%
  mutate(censoring = ifelse(medianHouseValue == 500001, 1, 0)) %>%
  dplyr::select(-id, -medianHouseValue) %>%
  mutate(oceanProximity = as.factor(oceanProximity))


```

## 2.2.2 Two classification methods

\noindent \large \textbf{Logistic Regression:} \normalsize

\noindent In this case logistic regression can model the log-odds of hitting the $\$500,001$ cap (censoring $= 1$) as a linear function of the predictors: \vspace{-1em}

$$\log \frac{Pr(\text{censor} = 1 | X_1,...,X_p)}{1 - Pr(\text{censor} = 1 | X_1,...,X_p)} = \beta_{0} + \beta_{1} \ \text{medianIncome} + \beta_{2} \ \text{housingMedianAge} + ...$$ 
This method is reasonable because:
\begin{enumerate}
  \item \textbf{Ideal for binary classification problems}: Designed specifically for binary response variables, logistic regression is suitable since in this case censoring take values of either $1$ (censored) or $0$ (not censored).
  \item \textbf{No strong distributional assumptions on }$\boldsymbol{X}$: Some of our variables may be skewed or heteroskedastic; logistic   regression simply cares that the logit is linear in $\boldsymbol{X}$ (where $\boldsymbol{X} = (X_1, ..., X_p)$).
  \item \textbf{Interpretable effects}: e.g. $e^{\beta_{1}}$ tells us how one unit ($\$1,000$) increases in medianIncome multiplies the odds of a      house price being censored.
  \item \textbf{Flexibility}: It estimates the probability that a given input belongs to a particular class, which also provides us with the flexibility of choosing the threshold for which optimises the testing correct rate with the assistance of the ROC curve.
\end{enumerate}


\noindent \large \textbf{KNN-CV:} \normalsize

\noindent In this case KNN-CV is a non-parametric approach that classifies each block group by majority voting among its $k$ closest neighbours in the predictor space. Formally, for a test point $x_0$, let $N_0$ be the indices of the $k$ nearest training observations (by Euclidean distance):

$$Pr(\text{censor} = 1|x_0) = \frac{1}{k}\sum_{i\in N_0}I(\text{censor}_i = 1)$$

\noindent and we predict the class with the larger estimated probability. We choose $k$ via cross-validation to pick the optimal bias-variance trade-off. \\

\noindent This method is reasonable because:
\begin{enumerate}
  \item \textbf{Captures nonlinearity}: If censoring depends on complex interactions and requires a non-linear decision boundary, KNN can adapt without specifying a specific model form.
  \item \textbf{No parametric assumptions}: No distributional assumptions required--so it's robust to skewed, heteroskedastic variables.
  \item \textbf{Tunable bias-variance trade-off via cross-validation}: The ability to choose $k$ allows us to control the smoothness.
    \begin{itemize}
        \item Small $k \implies$ very flexible (low bias, high variance)
        \item Large $k \implies$ smoother (high bias, low variance)
        \item Using cross-validation empirically estimates the error for each $k$, letting us pick a $k$ value the minimises both over and under fitting
    \end{itemize}
\end{enumerate}

```{r}
# some alternatives to consider: 
```





## 2.2.3 Which classification method performs better?

\textbf{How we have used the given data:}
\begin{itemize}
  \item We used a random 80/20 training and test split of the dataset to produce the test error rates.
  \item We dropped the \textit{id} and \textit{medianHouseValue} columns to ensure we only use legitimate predictors when training and testing our     model.
  \item The logistic model thresholds at $0.5$, while KNN makes decisions based on the majority class among the nearest neighbours in the feature      space.
\end{itemize}

```{r}
# Logistic Regression without Model Improvements
train_data_c <- data_c[train_index, ]
test_data_c <- data_c[-train_index, ] 

logit_fit_1 <- glm(censoring ~ longitude + 
                   latitude + 
                   medianIncome + 
                   housingMedianAge + 
                   oceanProximity + 
                   aveRooms + 
                   aveBedrooms,
                 data = train_data_c, 
                 family = binomial)

logit_pred_1 <- as.numeric(predict(logit_fit_1, 
                      newdata = test_data_c, 
                      type = "response") > 0.5) # use 0.5 for now 

error_rate <- mean(logit_pred_1 != test_data_c$censoring)

print(paste("Logistic Regression Prediction Error Rate:", round(error_rate, 3)))

#library(caret)
# Confusion matrix 
confusion_matrix <- confusionMatrix(as.factor(logit_pred_1), as.factor(test_data_c$censoring))
print(confusion_matrix)

# ROC 
#library(caret)
#library(pROC)
logit_pred_probs_1 <- predict(logit_fit_1, newdata = test_data_c, type = "response")
roc_curve_1 <- roc(test_data_c$censoring, logit_pred_probs_1)
plot(roc_curve_1, main = "ROC Curve", col = "blue", lwd = 2)
# error rate: 0.031
```



```{r}
# Logistic Regression with Model Improvements
train_data_c_imp <- data_c_imp[train_index, ]
test_data_c_imp <- data_c_imp[-train_index, ] 

logit_fit_2 <- glm(censoring ~ longitude + 
                   latitude + 
                   medianIncome + 
                   housingMedianAge + 
                   oceanProximity + 
                   aveRooms + 
                   aveBedrooms +
                   cityProximityScore + 
                   bedroomsPerRoom + 
                   incomePerRoom +
                   distToSF +
                   distToLA +
                   cosDirToLA +
                   cosDirToSF +
                   sinDirToLA +
                   sinDirToSF,
                 data = train_data_c_imp, 
                 family = binomial)

logit_pred_2 <- as.numeric(predict(logit_fit_2, 
                      newdata = test_data_c_imp, 
                      type = "response") > 0.5) # use 0.5 for now 

error_rate <- mean(logit_pred_2 != test_data_c_imp$censoring)

print(paste("Logistic Regression Prediction Error Rate:", round(error_rate, 3)))

#library(caret)
# Confusion matrix 
confusion_matrix <- confusionMatrix(as.factor(logit_pred_2), as.factor(test_data_c_imp$censoring))
print(confusion_matrix)

# ROC 
#library(caret)
#library(pROC)
logit_pred_probs_2 <- predict(logit_fit_2, newdata = test_data_c_imp, type = "response")
roc_curve_2 <- roc(test_data_c_imp$censoring, logit_pred_probs_2)
plot(roc_curve_2, main = "ROC Curve", col = "blue", lwd = 2)
# error rate: 0.029
```


```{r}
## KNN-CV Without Model Improvements

# Prepare the predictors (X) and response (Y)
train.X <- model.matrix(~ longitude + 
                          latitude + 
                          housingMedianAge + 
                          aveRooms + 
                          aveBedrooms + 
                          population + 
                          medianIncome + 
                          oceanProximity, 
                        data = train_data_c)[, -1]

test.X <- model.matrix(~ longitude + 
                         latitude + 
                         housingMedianAge + 
                         aveRooms + 
                         aveBedrooms + 
                         population + 
                         medianIncome + 
                         oceanProximity, 
                       data = test_data_c)[, -1]

train.Y <- train_data_c$censoring
test.Y <- test_data_c$censoring

# Use cross-validation on the training set to choose the best k
knn_rp <- rep(0, 15)
for (k in 1:15) {
  set.seed(5)
  knn.pred.cv <- knn.cv(train = train.X, cl = train.Y, k = k)
  knn_rp[k] <- mean(knn.pred.cv == train.Y)
}

k.cv <- which.max(knn_rp)
cat("Best k selected by CV is:", k.cv, "\n")

# Fit and predict on the test set using the best k
set.seed(5)
knn_pred_c_1 <- knn(train = train.X, test = test.X, cl = train.Y, k = k.cv)

# Compute confusion matrix and error rate
confusionMatrix(as.factor(knn_pred_c_1), as.factor(test.Y))

(knn_err_c_1 = mean(knn_pred_c_1 != test.Y))
cat("The KNN error rate is", round(knn_err_c_1, 4), "\n")


#The KNN error rate is 0.049 

```

```{r}
## KNN-CV with Model Improvements

# Prepare the predictors (X) and response (Y)
train.X <- model.matrix(~ longitude + 
                          latitude + 
                          medianIncome + 
                          housingMedianAge + 
                          oceanProximity + 
                          aveRooms + 
                          aveBedrooms +
                          cityProximityScore + 
                          bedroomsPerRoom + 
                          incomePerRoom +
                          distToSF +
                          distToLA +
                          cosDirToLA +
                          cosDirToSF +
                          sinDirToLA +
                          sinDirToSF, 
                        data = train_data_c_imp)[, -1]

test.X <- model.matrix(~  longitude + 
                          latitude + 
                          medianIncome + 
                          housingMedianAge + 
                          oceanProximity + 
                          aveRooms + 
                          aveBedrooms +
                          cityProximityScore + 
                          bedroomsPerRoom + 
                          incomePerRoom +
                          distToSF +
                          distToLA +
                          cosDirToLA +
                          cosDirToSF +
                          sinDirToLA +
                          sinDirToSF, 
                       data = test_data_c_imp)[, -1]

train.Y <- train_data_c_imp$censoring
test.Y <- test_data_c_imp$censoring

# Use cross-validation on the training set to choose the best k
knn_rp <- rep(0, 15)
for (k in 1:15) {
  set.seed(5)
  knn.pred.cv <- knn.cv(train = train.X, cl = train.Y, k = k)
  knn_rp[k] <- mean(knn.pred.cv == train.Y)
}

k.cv <- which.max(knn_rp)
cat("Best k selected by CV is:", k.cv, "\n")

# Fit and predict on the test set using the best k
set.seed(5)
knn_pred_c_2 <- knn(train = train.X, test = test.X, cl = train.Y, k = k.cv)

# Compute confusion matrix and error rate
confusionMatrix(as.factor(knn_pred_c_2), as.factor(test.Y))

(knn_err_c_2 = mean(knn_pred_c_2 != test.Y))
cat("The KNN error rate is", round(knn_err_c_2, 4), "\n")


#The KNN error rate is 0.0306 

```

## 2.3.4 Justifying the better classification method

In our 80/20 hold-out test we observed:

- Alina justification here


# 2.3 A hybrid approach

## 2.3.1 Dicussing feasibility of the approach
Estimate using improved KNN and then apply the censoring from logistic.
More explanation necessary

## 2.3.2 test MSE of medianHousingValue using the approach

```{r}
# Step 1: Predict house prices with KNN regression
knn_pred_2 <- predict(knn_reg_model_2, newdata = test_data_encoded)

# Step 2: Predict censoring with logistic regression
logit_pred_2 <- as.numeric(predict(logit_fit_2, newdata = test_data_c_imp, type = "response") > 0.5)

# Step 3: Adjust predicted house prices
# If censored predicted, clip to 500001
adjusted_preds <- ifelse(logit_pred_2 == 1, 500001, knn_pred_2)

# Step 4: Compute MSE using all observations
final_mse <- mean((adjusted_preds - test_data_encoded$medianHouseValue)^2)

cat("Final Test MSE:", round(final_mse, 2), "\n")


# Final Test MSE: 3025628814 
# this is worse then KNN_2
```

## 2.3.3 Comparison of the accuracy of this procedure to model in 2.1.3

No test MSE for combined model is worse.

Other note, random test/train sample too. 

The combined model clips predictions to 500001 when the logistic model predicts censoring, but if that prediction is wrong, it forces the value far from the true (uncensored) price, increasing error. KNN, on the other hand, always predicts a value — even for censored cases — and sometimes lands closer to the unknown true price (which is at least 500001), purely by chance. This makes KNN appear to perform better in terms of MSE, even though it doesn't account for censoring and might be underestimating the true values.
