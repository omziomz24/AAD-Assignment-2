---
title: "Group-26 AAD-Assignment-2"
author: "Omar, Eloise, Alina, Sue"

date: "`r Sys.Date()`"
  html_document:
    toc: true
    toc_depth: '4'
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      dev = "png",
                      dpi = 150)

# Load relevant libraries
library(dplyr)
library(ggplot2)

library(gam) # for fitting gam models
library(caret)
library(tidyverse)
library(mgcv)

library(MASS) #LDA 
```

# 2.1 Nonlinear model vs non-parametric model



## 2.1.1 GAM vs KNN approach


\noindent \large \textbf{GAM:} \normalsize

\noindent \textbf{Pros}
\begin{itemize}
  \item \textbf{Flexibility to capture non-linearity}. In a GAMM we replace each linear term $\beta_{k} x_{k}$ with a smooth function $f_{k}(x_{k})$,     meaning we can automatically model non-linear relationships between each predictor and the response without having to manually try out different      transformation on each variable individually.
  \item \textbf{Non-linear fits may be more accurate}. The ability of creating this non-linear predictor and response relationship may make our model     more accurate when predicting the medianHousingValue.
  \item \textbf{Interperability from additivity}. Since a GAMM is additive, we can examine the effect of each $X_j$ on $Y$ individually while holding      all of the other variables fixed. Therefore, we can understand each variables individual effect on house value.
  \item \textbf{Control over smoothness}. Each $f_j$ comes with an associated smoothing parameter (or degrees of freedom), making it straightforward      to trade bias and variance  (e.g. via cross-validation).
\end{itemize}

\noindent \textbf{Cons}
\begin{itemize}
  \item \textbf{Additivity assumption restriction}. If there are strong synergistic effects like between location (longitude/latitude) and income         \textendash an additive model will not capture them unless explicit interaction terms $X_j \times X_k$ are included or low-dimensional               interactions function of the form $f_{jk}(X_j, X_k)$ are manually introduced. 
  \item \textbf{Computational cost}. Fitting this model to over \textasciitilde$20,600$ observations and multiple smoothers can be very slow when        compared to fitting a single parametric method.
\end{itemize}

\noindent \large \textbf{KNN:} \normalsize

\noindent \textbf{Pros}
\begin{itemize}
  \item \textbf{Completely non-parametric}. KNN makes no assumptions about the form of $f(X)$, allowing the model to potentially fit better than a      parametric model. At a point $x_0$ KNN averages the responses of the $K$ closest training blocks: $\hat{f}(x_{0}) = \frac{1}{K} \sum_{x_{i}           \in N_{0}}{y_i}$, where $N_0$ is the set of the $K$ nearest neighbors.
  \item \textbf{Control of bias-variance}. A small $K$ yields a very flexible, low-bias but high-variance fit; large $K$ yields a smoother,      lower-variance fit.
\end{itemize}

\noindent \textbf{Cons}
\begin{itemize}
  \item \textbf{Dimensoniality constraints}. As the number of predictors grows, the "nearest" neighbors tend to be far away in a high-dimensional       space, so KNN's performance degrades rapidly as the number of predictors grow.
  \item \textbf{Distance metric sensitivity}. With a KNN you must scale the numeric features and encode the categorical features (e.g.                  oceanProximity) carefully. Otherwise poorly scaled or encoded features can dominate the distance of the nearest neighbors calculation.
  \item \textbf{Computationally intensive with predictions}. For each new group of predictions all \textasciitilde$20,600$ must be computed to find     the $K$ nearest, which can be very intensive and slow.
  \item \textbf{Low interperability}. There is no simple way to explain a KNN prediction beyond pointing to the raw neighbors and their average.
\end{itemize}

\noindent \large \textbf{In this housing-price context: } \normalsize
\begin{itemize}
  \item \textbf{GAMM:} it is likely to give an interpretable model with, as we can analyse the partial-effects (e.g. how median income or ocean         proximity individually affects price), and we can capture smooth non-linear trends.
  \item \textbf{KNN:} can capture complex interactions, between predictors, automatically, but with eight predictors (including a categorical     one)   it may run into high-dimensionality issues, making distance-based averaging unstable and slow.
\end{itemize} 





## 2.1.2 GAM vs KNN Regression Model

```{r}
# install.packages(c("caret", "tidyverse"))
library(caret)
library(tidyverse)
```


# OMAR CODE 
```{r}

# Load data and clean NA
data <- read.csv("Assignt2_data.csv")
data <- na.omit(data)

# Split data into training (70%) and test (30%) sets
set.seed(26)
n <- nrow(data)
train_index <- sample(1:n, size=0.7*n)
data_train <- data[train_index, ]
data_test <- data[-train_index, ]

# Fit GAM model using smoothing spline (df=4 for now) on each continuous predictor
gam_fit <- gam(
  medianHouseValue ~ 
    s(longitude, df = 4) +
    s(latitude, df = 4) +
    s(housingMedianAge, df = 4) +
    s(aveRooms, df = 4) +
    s(aveBedrooms, df = 4) +
    s(population, df = 4) +
    s(medianIncome, df = 4),
  data=data_train
)
#par(mfrow = c(1, 4))
#plot.Gam(gam_fit, se=TRUE, col="blue")

preds_gam <- predict(gam_fit, newdata = data_test)
gam_mse <- mean((data_test$medianHouseValue - preds_gam)^2)
gam_mse

# Fit KNN model
```
# END OMAR CODE

# START SUE CODE

We will use the cleaned data (omitting all data point that contains "NA") instead of the original raw dataset. We split the data into a training set (80% of the original data) and a test set. 

```{r}
# load the data 
data <- read.csv("Assignt2_data.csv")
data <- na.omit(data)
# View(data)
```


```{r}
# separate the train and test set out 
set.seed(1)
train_index <- createDataPartition(data$medianHouseValue, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data  <- data[-train_index, ]
```


```{r}
# KNN 

  # model improvements including pca and cv for k selection 
knn_reg_model <- train(
  medianHouseValue ~ ., 
  data = train_data,
  method = "knn",
  preProcess = c("center", "scale", "pca"),  # scales & applies PCA 
  tuneGrid = data.frame(k = 1:10),  # test k = 1 to 10
  trControl = trainControl(method = "cv", number = 5) 
) 

# best k 
print(knn_reg_model)

# KNN MSE calculation 
knn_pred <- predict(knn_reg_model, newdata = test_data)
knn_MSE <- mean((knn_pred - test_data$medianHouseValue)^2) 
cat("The KNN MSE is", knn_MSE, "\n")
```

```{r}
# GAM 

# install.packages("mgcv")
library(mgcv)

# the split is the same as before 

# train the GAM 
gam_model <- gam(medianHouseValue ~ s(latitude) + s(longitude) + 
                   s(housingMedianAge) + s(aveRooms) +
                   s(aveBedrooms) + s(population) +
                   s(medianIncome), data = train_data)

# Calculate MSE 
gam_pred <- predict(gam_model, newdata = test_data)
gam_MSE <- mean((gam_pred - test_data$medianHouseValue)^2)
cat("The GAM MSE is", gam_MSE, "\n")
```

```{r}
# GAM improvements 

gam.check(gam_model) # 
summary(gam_model)

# suspect high corr between longitude and latitude 
 # should add interaction term 
plot(data$longitude, data$latitude)

# same thing with aveRooms and aveBedrooms 
plot(data$aveRooms, data$aveBedrooms)
```
###### some justifications required: 

```{r} 
# Using the predictors from Asmt1 ? 

# bedroomsPerRoom
data$bedroomsPerRoom <- data$aveBedrooms / data$aveRooms

# distance to centre
center_lat <- mean(data$latitude)
center_lon <- mean(data$longitude)
# data$distToCenter <- sqrt((data$latitude - center_lat)^2 + (data$longitude - center_lon)^2)

# incomePerRoom
data$incomePerRoom = data$medianIncome / data$aveRooms

# Dist from LA and SF
library(geosphere)
la_coords <- c(-118.24, 34.05)   # (longitude, latitude)
sf_coords <- c(-122.42, 37.77)
distToLA <- apply(data[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, la_coords) / 1000  # convert meters to km
}) # Add distance to LA
distToSF <- apply(data[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, sf_coords) / 1000
}) # Add distance to SF
# Compute direction angles
dirToLA <- atan2(data$latitude - la_coords[2], data$longitude - la_coords[1])
dirToSF <- atan2(data$latitude - sf_coords[2], data$longitude - sf_coords[1])

# City Proximity 
cosDirToLA <- cos(dirToLA)
sinDirToLA <- sin(dirToLA)
cosDirToSF <- cos(dirToSF)
sinDirToSF <- sin(dirToSF)
data$cityProximityScore <- 1 / (1 + distToLA) + 1 / (1 + distToSF)

# View(data)
```

```{r}
train_data = data[train_index, ]
gam_model_2 <- gam(medianHouseValue ~ s(latitude) + 
                     s(longitude) + 
                     s(housingMedianAge) + 
                     s(aveRooms) +
                     s(aveBedrooms) + 
                     s(population) + 
                     s(medianIncome) + 
                     factor(oceanProximity) + 
                     te(longitude, latitude) + 
                     s(cityProximityScore) + 
                     s(bedroomsPerRoom) + 
                     s(incomePerRoom) 
                     ,  
                   data = train_data,
                   select = TRUE) # to reduce overfitting 


# Calculate MSE 
test_data = data[-train_index, ]
gam_pred_2 <- predict(gam_model_2, newdata = test_data)
gam_MSE_2 <- mean((gam_pred_2 - test_data$medianHouseValue)^2)
cat("The GAM MSE (improved) is", gam_MSE_2, "\n")
```

# END SUE CODE 


## 2.1.3 Which model performs better?

```{r}
# if any improvements to 2.1.2 occurs and changes the result between KNN and GAM, we need to update the following analysis.
```

In this regression problem, even after improvements, the KNN model outperforms the GAM model as shown by its lower test MSE (approximately 3.91 billion versus 4.10 billion). This indicates that KNN better captures the complex, nonlinear relationships in the data.

KNN does not seem to suffer significantly from the curse of dimensionality here. Its ability to accommodate complex relationships is enhanced by integrating PCA, which effectively reduces bias with only a small increase in variance. This balance allows KNN to generalize well on the test data after tuning the number of neighbors.

For the GAM model, attempts to incorporate additional variables (e.g. cityProximityScore) that potentially capture more relevant information have resulted in only marginal improvements in MSE. This suggests that the additive smooth terms in GAM are still too restrictive and unable to fully model the underlying data complexity, limiting its predictive power.

Overall, these results reflect the bias-variance trade-off: KNN achieves lower bias through flexible local fitting and dimensionality reduction, while GAM’s higher bias—due to its smoother additive assumptions—limits its performance despite generally lower variance.



# 2.2 Classification models

## 2.2.1 New censoring boolean column
```{r}
# Load original dataset and clean for NA
data <- read.csv("Assignt2_data.csv")
data <- na.omit(data)

# Make a new column "censoring" where 1 if medianHouseValue=500,001; and 0 otherwise
data <- data %>%
  mutate(censoring = ifelse(medianHouseValue == 500001, 1, 0))

# what about this data$censoring <- as.integer(data$medianHouseValue==500001)
```

## 2.2.2 Two classification methods
!!!!!!!!!!!!!
IM NOT TOO SURE HERE! Idk wether to use KNN classification or LDA? cause LDA is only good if n is small and the distribution of the predictor is approximately normal which it isnt most likely
!!!!!!!!!!!!

-- I think LDA and QDA were introduced because they are suitable for non-binary response? 
-- What about TREE and SVM ? 

\noindent \large \textbf{Logistic Regression:} \normalsize

\noindent In this case logistic regression can model the log-odds of hitting the $\$500,001$ cap (censoring $= 1$) as a linear function of the predictors:

$$\log \frac{\mathbb{P}(\text{censor} = 1 | X)}{1 - \mathbb{P}(\text{censor} = 1 | X)} = \beta_{0} + \beta_{1} \ \text{medianIncome} + \beta_{2} \ \text{housingMedianAge} + ...$$ 
This method is reasonable because:
\begin{enumerate}
  \item \textbf{Ideal for binary classification problems}: Designed specifically for binary response variables, logistic regression is suitable since in this case censoring take values of either $1$ (censored) or $0$ (not censored). 
  \item \textbf{Handles mixed feature types:} This method works with our continuous predictor and categorical predictors (oceanProximity)
  \item \textbf{No strong distributional assumptions on }$\boldsymbol{X}:$\textbf{:} Some of our variables may be skewed or heteroskedastic; logistic   regression simply cares that the logit is linear in $X$, not that $X | Y$ is normal.
  \item \textbf{Interpretable effects:} e.g. $e^{\beta_{1}}$ tells us how one unit ($\$1,000$) increases in medianIncome multiplies the odds of a      house price being censored.
  \item \textbf{Flexibility}: It estimates the probability that a given input belongs to a particular class, which also provides us with the flexibility of choosing the threshold for which optimises the testing correct rate with the assistance of ROC table.
\end{enumerate}


\noindent \large \textbf{Linear Discriminant Analysis (LDA):} \normalsize

\noindent In this case LDA assumes that, within each class (censor $= 0$ or $1$), the vector of predictors $($medianIncome,housingMedianAge,...$)^\top$ is drawn from a multivariate Normal with class-specific mean $\mu_{k}$ but a common covariance $\boldsymbol{\Sigma}$. It then classifies via the linear discriminant:

$$\delta_k(x) = x^T \boldsymbol{\Sigma^{-1}} \mu_k - \frac{1}{2} \mu_k^T \boldsymbol{\Sigma^{-1}} \mu_k + \log \pi_k$$
\noindent assigning each $x$ to a class with the largest $\delta_{k}$ 

This method is reasonable because:
\begin{enumerate}
  \item \textbf{Captures covariance structure:} between some of the variables there is correlation (e.g. high-income also tends to have more bedrooms   )\textendash and that joint pattern differs between censored vs uncensored data\textenddash LDA leverages those correlations in a single rule.
  \item \textbf{Transparent boundary:} 
\end{enumerate}

```{r}
# some alternatives to consider: 
```

\noindent \large \textbf{Support Vector Machine (SVM)} \normalsize

\begin{enumerate} 
\item \textbf{Effevtive for high-dimensionality}: Since we have a large amount of features, SVM are suitable due to its kernel approach. 
\item \textbf{Avoids ovefitting by maximising margin}: SVM focuses on maximizing the margin between the classes (i.e. it tries to be as far away as possible from the closest points in each class (called support vectors)). This reduces the model's sensitivity to small changes in data, which helps the model perform better on unseen data.
\item \textbf{Accomodating non-linear relationships}: SVMs can be extended to handle complex decision boundaries through the kernel appraoch, allowing modeling of nonlinear relationships.





## 2.2.3 Which classification method performs better?

```{r}
# Logistic Regression 

train_data <- data[train_index, ]
test_data <- data[-train_index, ] 

logit_fit <- glm(censoring ~ longitude + 
                   latitude + 
                   medianIncome + 
                   housingMedianAge + 
                   oceanProximity + 
                   aveRooms + 
                   aveBedrooms,
                 data = train_data, 
                 family = binomial)

logit_pred <- as.numeric(predict(logit_fit, 
                      newdata = test_data, 
                      type = "response") > 0.5) # use 0.5 for now 

error_rate <- mean(logit_pred != test_data$censoring)

print(paste("Logistic Regression Prediction Error Rate:", round(error_rate, 3)))
# wait this is extremely low - worried about data leakage need to come back !!! \


library(caret)
# Confusion matrix 
confusion_matrix <- confusionMatrix(as.factor(logit_pred), as.factor(test_data$censoring))
print(confusion_matrix)

# ROC 
# install.packages("caret")
library(caret)
# install.packages("pROC")
library(pROC)
logit_pred_probs <- predict(logit_fit, newdata = test_data, type = "response")
roc_curve <- roc(test_data$censoring, logit_pred_probs)
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
```


```{r}
# LDA 
# library(MASS)

lda_fit <- lda(censoring ~ longitude + 
                 latitude + 
                 housingMedianAge + 
                 aveRooms + 
                 aveBedrooms + 
                 population + 
                 medianIncome + 
                 oceanProximity, 
               data = train_data)

lda_pred <- predict(lda_fit, test_data)$class 

# library(caret)
confusionMatrix(as.factor(lda_pred), as.factor(test_data$censoring))

(lda_err = mean(lda_pred != test_data$censoring)) 
cat("The LDA error rate is", round(lda_err, 4), "\n")
```

```{r}
# SVM 

# install.packages("e1071")
library(e1071)
train_data <- data[train_index, ] # in case any modification of the data set needed gotta rerun this 
test_data <- data[-train_index, ]

svm_model <- svm(censoring ~ longitude + latitude + medianIncome + housingMedianAge + 
                   oceanProximity + aveRooms + aveBedrooms, 
                 data = train_data, 
                 kernel = "linear",  # Linear kernel (you can try other kernels like "radial" or "polynomial")
                 cost = 1,          # Regularization parameter (tune this)
                 scale = TRUE)      # Standardize the predictors
```





# 2.3 A hybrid approach

## 2.3.1 Dicussing feasibility of the approach

## 2.3.2 test MSE of medianHousingValue using the approach

## 2.3.3 Comparison of the accuracy of this procedure to model in 2.1.3

