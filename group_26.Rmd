---
title: "Group-26 AAD-Assignment-2"
author: "Omar, Eloise, Alina, Sue"

date: "`r Sys.Date()`"
  html_document:
    toc: true
    toc_depth: '4'
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      dev = "png",
                      dpi = 150)

# Load relevant libraries
library(dplyr)
library(ggplot2)
#library(gam)
library(caret)
library(tidyverse)
library(mgcv) # for GAM
library(MASS) # for LDA 
library(class) # for KNN
library(geosphere)
library(pROC)
```

# 2.1 Nonlinear model vs non-parametric model



## 2.1.1 GAM vs KNN approach


\noindent \large \textbf{GAM:} \normalsize

\noindent \textbf{Pros}
\begin{itemize}
  \item \textbf{Flexibility to capture non-linearity}. In a GAMM we replace each linear term $\beta_{k} x_{k}$ with a smooth function $f_{k}(x_{k})$,     meaning we can automatically model non-linear relationships between each predictor and the response without having to manually try out different      transformation on each variable individually.
  \item \textbf{Non-linear fits may be more accurate}. The ability of creating this non-linear predictor and response relationship may make our model     more accurate when predicting the medianHousingValue.
  \item \textbf{Interperability from additivity}. Since a GAMM is additive, we can examine the effect of each $X_j$ on $Y$ individually while holding      all of the other variables fixed. Therefore, we can understand each variables individual effect on house value.
  \item \textbf{Control over smoothness}. Each $f_j$ comes with an associated smoothing parameter (or degrees of freedom), making it straightforward      to trade bias and variance  (e.g. via cross-validation).
\end{itemize}

\noindent \textbf{Cons}
\begin{itemize}
  \item \textbf{Additivity assumption restriction}. If there are strong synergistic effects like between location (longitude/latitude) and income         \textendash an additive model will not capture them unless explicit interaction terms $X_j \times X_k$ are included or low-dimensional               interactions function of the form $f_{jk}(X_j, X_k)$ are manually introduced. 
  \item \textbf{Computational cost}. Fitting this model to over \textasciitilde$20,600$ observations and multiple smoothers can be very slow when        compared to fitting a single parametric method.
\end{itemize}

\noindent \large \textbf{KNN:} \normalsize

\noindent \textbf{Pros}
\begin{itemize}
  \item \textbf{Completely non-parametric}. KNN makes no assumptions about the form of $f(X)$, allowing the model to potentially fit better than a      parametric model. At a point $x_0$ KNN averages the responses of the $K$ closest training blocks: $\hat{f}(x_{0}) = \frac{1}{K} \sum_{x_{i}           \in N_{0}}{y_i}$, where $N_0$ is the set of the $K$ nearest neighbors.
  \item \textbf{Control of bias-variance}. A small $K$ yields a very flexible, low-bias but high-variance fit; large $K$ yields a smoother,      lower-variance fit.
\end{itemize}

\noindent \textbf{Cons}
\begin{itemize}
  \item \textbf{Dimensoniality constraints}. As the number of predictors grows, the "nearest" neighbors tend to be far away in a high-dimensional       space, so KNN's performance degrades rapidly as the number of predictors grow.
  \item \textbf{Distance metric sensitivity}. With a KNN you must scale the numeric features and encode the categorical features (e.g.                  oceanProximity) carefully. Otherwise poorly scaled or encoded features can dominate the distance of the nearest neighbors calculation.
  \item \textbf{Computationally intensive with predictions}. For each new group of predictions all \textasciitilde$20,600$ must be computed to find     the $K$ nearest, which can be very intensive and slow.
  \item \textbf{Low interperability}. There is no simple way to explain a KNN prediction beyond pointing to the raw neighbors and their average.
\end{itemize}

\noindent \large \textbf{In this housing-price context: } \normalsize
\begin{itemize}
  \item \textbf{GAMM:} it is likely to give an interpretable model with, as we can analyse the partial-effects (e.g. how median income or ocean         proximity individually affects price), and we can capture smooth non-linear trends.
  \item \textbf{KNN:} can capture complex interactions, between predictors, automatically, but with eight predictors (including a categorical     one)   it may run into high-dimensionality issues, making distance-based averaging unstable and slow.
\end{itemize} 





## 2.1.2 GAM vs KNN Regression Model



We will use the cleaned data (omitting all data point that contains "NA") instead of the original raw dataset. We split the data into a training set (80% of the original data) and a test set (20% of the original data). 

```{r}
# load the data 
data <- read.csv("Assignt2_data.csv")
data <- na.omit(data)
# View(data)
```


```{r}
# separate the train and test set out 
set.seed(1)
train_index <- createDataPartition(data$medianHouseValue, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data  <- data[-train_index, ]
```


```{r}
# Basic KNN
knn_reg_model_1 <- train(
  medianHouseValue ~ ., 
  data = train_data,
  method = "knn",
  preProcess = c("center", "scale"),
  tuneGrid = data.frame(k = 1:10),  # test k = 1 to 10
  trControl = trainControl(method = "cv", number = 5) 
) 

# best k 
print(knn_reg_model_1)

# KNN MSE calculation 
knn_pred_1 <- predict(knn_reg_model_1, newdata = test_data)
knn_MSE_1 <- mean((knn_pred_1 - test_data$medianHouseValue)^2) 
cat("The KNN MSE is", knn_MSE_1, "\n")
<<<<<<< HEAD

=======
# The KNN MSE is 3697939911 
>>>>>>> main
```

### Model Improvements

```{r}
# Using the predictors from Asmt1 ? 
data_imp <- read.csv("Assignt2_data.csv")
data_imp <- na.omit(data_imp)

# bedroomsPerRoom
data_imp$bedroomsPerRoom <- data_imp$aveBedrooms / data_imp$aveRooms


# incomePerRoom
data_imp$incomePerRoom = data_imp$medianIncome / data_imp$aveRooms

# Compute distances
la_coords <- c(-118.24, 34.05)   # (longitude, latitude)
sf_coords <- c(-122.42, 37.77)

data_imp$distToLA <- apply(data_imp[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, la_coords) / 1000
})
data_imp$distToSF <- apply(data_imp[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, sf_coords) / 1000
})

# Compute directions
data_imp$dirToLA <- atan2(data_imp$latitude - la_coords[2], data_imp$longitude - la_coords[1])
data_imp$dirToSF <- atan2(data_imp$latitude - sf_coords[2], data_imp$longitude - sf_coords[1])

# Cosine and sine of direction
data_imp$cosDirToLA <- cos(data_imp$dirToLA)
data_imp$sinDirToLA <- sin(data_imp$dirToLA)
data_imp$cosDirToSF <- cos(data_imp$dirToSF)
data_imp$sinDirToSF <- sin(data_imp$dirToSF)
data_imp$dirToLA <- NULL
data_imp$dirToSF <- NULL

# Composite proximity score
data_imp$cityProximityScore <- 1 / (1 + data_imp$distToLA) + 1 / (1 + data_imp$distToSF)

# Add these to test and train
train_data_imp <- data_imp[train_index, ]
test_data_imp  <- data_imp[-train_index, ]




```



```{r}
# with model improvements
# Hot encoding the variables

# Convert oceanProximity to factor (if not already)
train_data_imp$oceanProximity <- as.factor(train_data_imp$oceanProximity)
test_data_imp$oceanProximity <- as.factor(test_data_imp$oceanProximity)

# One-hot encode using dummyVars
dummies <- dummyVars(~ ., data = train_data_imp)
train_data_encoded <- predict(dummies, newdata = train_data_imp)
test_data_encoded <- predict(dummies, newdata = test_data_imp)

# Convert to data frame
train_data_encoded <- as.data.frame(train_data_encoded)
test_data_encoded <- as.data.frame(test_data_encoded)



knn_reg_model_2 <- train(
  medianHouseValue ~ ., 
  data = train_data_encoded,
  method = "knn",
  preProcess = c("center", "scale"),
  tuneGrid = data.frame(k = 1:15),  # test k = 1 to 15
  trControl = trainControl(method = "cv", number = 5) 
) 

# best k 
print(knn_reg_model_2)

# KNN MSE calculation 
knn_pred_2 <- predict(knn_reg_model_2, newdata = test_data_encoded)
knn_MSE_2 <- mean((knn_pred_2 - test_data_encoded$medianHouseValue)^2) 
cat("The KNN MSE (improved) is", knn_MSE_2, "\n")


#The KNN MSE is 3697939911 with out encoding
#With encoding  3694531899
#PCA makes it worse in general
#improve when tested more k 3650562433
# best now is: 2999761722 
```


```{r}
# GAM 
# the split is the same as before 

# train the GAM 
gam_model_1 <- mgcv::gam(medianHouseValue ~ s(latitude) + s(longitude) + 
                   s(housingMedianAge) + s(aveRooms) +
                   s(aveBedrooms) + s(population) +
                   s(medianIncome) + factor(oceanProximity), data = train_data)

# Calculate MSE 
gam_pred_1 <- predict(gam_model_1, newdata = test_data)
gam_MSE_1 <- mean((gam_pred_1 - test_data$medianHouseValue)^2)
cat("The GAM MSE is", gam_MSE_1, "\n")

#4258008383 
```

###### some justifications required: 

```{r}
# GAM improvements
train_data_imp = data_imp[train_index, ]
gam_model_2 <- mgcv::gam(medianHouseValue ~ 
                     s(housingMedianAge) + 
                     s(aveRooms) +
                     s(aveBedrooms) + 
                     s(population) + 
                     s(medianIncome) +
                     te(longitude, latitude) +
                     s(cityProximityScore) + 
                     s(bedroomsPerRoom) + 
                     s(incomePerRoom) +
                     s(distToSF) +
                     s(distToLA) +
                     s(cosDirToLA) +
                     s(cosDirToSF) +
                     s(sinDirToLA) +
                     s(sinDirToSF),  
                   data = train_data_imp,
                   select = TRUE) # to reduce overfitting 


# Calculate MSE 
test_data_imp = data_imp[-train_index, ]
gam_pred_2 <- predict(gam_model_2, newdata = test_data_imp)
gam_MSE_2 <- mean((gam_pred_2 - test_data_imp$medianHouseValue)^2)
cat("The GAM MSE (improved) is", gam_MSE_2, "\n")

#3386284239 
```


## 2.1.3 Which model performs better?

```{r}
# if any improvements to 2.1.2 occurs and changes the result between KNN and GAM, we need to update the following analysis.
```

In this regression problem, even after improvements, the KNN model outperforms the GAM model as shown by its lower test MSE (approximately 3.00 billion versus 3.34 billion). This indicates that KNN better captures the complex, nonlinear relationships in the data.

KNN does not appear to suffer from the curse of dimensionality in this case, even with a relatively high number of predictors. After tuning the number of neighbors, the model captures complex relationships in the data without overfitting. Its strong test performance suggests that the added predictors did not harm generalisation.

For the GAM model, attempts to incorporate additional variables (e.g. cityProximityScore) that potentially capture more relevant information have resulted in only marginal improvements in MSE. This suggests that the additive smooth terms in GAM are still too restrictive and unable to fully model the underlying data complexity, limiting its predictive power.

Overall, these results reflect the bias-variance trade-off: KNN achieves lower bias through flexible local fitting and dimensionality reduction, while GAM’s higher bias—due to its smoother additive assumptions—limits its performance despite generally lower variance.



# 2.2 Classification models

## 2.2.1 New censoring boolean column
```{r}


# Add censoring column and drop id and medianHouseValue
data_c <- data %>%
  na.omit() %>%
  mutate(censoring = ifelse(medianHouseValue == 500001, 1, 0)) %>%
  dplyr::select(-id, -medianHouseValue) %>%
  mutate(oceanProximity = as.factor(oceanProximity))

<<<<<<< HEAD
=======

# This dataframe does the same changes as above but is with our model improvements
>>>>>>> main
data_c_imp <- data_imp %>%
  na.omit() %>%
  mutate(censoring = ifelse(medianHouseValue == 500001, 1, 0)) %>%
  dplyr::select(-id, -medianHouseValue) %>%
  mutate(oceanProximity = as.factor(oceanProximity))


```

## 2.2.2 Two classification methods
<<<<<<< HEAD

=======
>>>>>>> main

\noindent \large \textbf{Logistic Regression:} \normalsize

\noindent In this case logistic regression can model the log-odds of hitting the $\$500,001$ cap (censoring $= 1$) as a linear function of the predictors: \vspace{-1em}

$$\log \frac{Pr(\text{censor} = 1 | X_1,...,X_p)}{1 - Pr(\text{censor} = 1 | X_1,...,X_p)} = \beta_{0} + \beta_{1} \ \text{medianIncome} + \beta_{2} \ \text{housingMedianAge} + ...$$ 
This method is reasonable because:
\begin{enumerate}
  \item \textbf{Ideal for binary classification problems}: Designed specifically for binary response variables, logistic regression is suitable since in this case censoring take values of either $1$ (censored) or $0$ (not censored).
  \item \textbf{No strong distributional assumptions on }$\boldsymbol{X}$: Some of our variables may be skewed or heteroscedastic; logistic   regression simply cares that the logit is linear in $\boldsymbol{X}$ (where $\boldsymbol{X} = (X_1, ..., X_p)$).
  \item \textbf{Interpretable effects}: e.g. $e^{\beta_{1}}$ tells us how one unit ($\$1,000$) increases in medianIncome multiplies the odds of a      house price being censored.
  \item \textbf{Flexibility}: It estimates the probability that a given input belongs to a particular class, which also provides us with the flexibility of choosing the threshold for which optimises the testing correct rate with the assistance of the ROC curve.
\end{enumerate}


\noindent \large \textbf{KNN-CV:} \normalsize

\noindent In this case KNN-CV is a non-parametric approach that classifies each block group by majority voting among its $k$ closest neighbours in the predictor space. Formally, for a test point $x_0$, let $N_0$ be the indices of the $k$ nearest training observations (by Euclidean distance):

$$Pr(\text{censor} = 1|x_0) = \frac{1}{k}\sum_{i\in N_0}I(\text{censor}_i = 1)$$

\noindent and we predict the class with the larger estimated probability. We choose $k$ via cross-validation to pick the optimal bias-variance trade-off. \\

\noindent This method is reasonable because:
\begin{enumerate}
  \item \textbf{Captures nonlinearity}: If censoring depends on complex interactions and requires a non-linear decision boundary, KNN can adapt without specifying a specific model form.
  \item \textbf{No parametric assumptions}: No distributional assumptions required--so it's robust to skewed, heteroskedastic variables.
  \item \textbf{Tunable bias-variance trade-off via cross-validation}: The ability to choose $k$ allows us to control the smoothness.
    \begin{itemize}
        \item Small $k \implies$ very flexible (low bias, high variance)
        \item Large $k \implies$ smoother (high bias, low variance)
        \item Using cross-validation empirically estimates the error for each $k$, letting us pick a $k$ value the minimises both over and under fitting
    \end{itemize}
\end{enumerate}

<<<<<<< HEAD
```{r}
# some alternatives to consider: 
```



=======
>>>>>>> main


## 2.2.3 Which classification method performs better?

\textbf{How we have used the given data:}
\begin{itemize}
  \item We used a random 80/20 training and test split of the dataset to produce the test error rates.
  \item We dropped the \textit{id} and \textit{medianHouseValue} columns to ensure we only use legitimate predictors when training and testing our     model.
<<<<<<< HEAD
  \item The logistic model thresholds at $0.5$, while KNN makes decisions based on the majority class among the nearest neighbors in the feature space.
\end{itemize}

```{r}
# Logistic Regression
=======
  \item The logistic model thresholds at $0.5$, while KNN makes decisions based on the majority class among the nearest neighbours in the feature      space.
\end{itemize}

```{r}
# Logistic Regression without Model Improvements
>>>>>>> main
train_data_c <- data_c[train_index, ]
test_data_c <- data_c[-train_index, ] 

logit_fit_1 <- glm(censoring ~ longitude + 
                   latitude + 
                   medianIncome + 
                   housingMedianAge + 
                   oceanProximity + 
                   aveRooms + 
                   aveBedrooms,
                 data = train_data_c, 
                 family = binomial)

logit_pred_1 <- as.numeric(predict(logit_fit_1, 
                      newdata = test_data_c, 
                      type = "response") > 0.5) # use 0.5 for now 

error_rate <- mean(logit_pred_1 != test_data_c$censoring)

print(paste("Logistic Regression Prediction Error Rate:", round(error_rate, 3)))

#library(caret)
# Confusion matrix 
confusion_matrix <- confusionMatrix(as.factor(logit_pred_1), as.factor(test_data_c$censoring))
print(confusion_matrix)

# ROC 
#library(caret)
#library(pROC)
logit_pred_probs_1 <- predict(logit_fit_1, newdata = test_data_c, type = "response")
roc_curve_1 <- roc(test_data_c$censoring, logit_pred_probs_1)
plot(roc_curve_1, main = "ROC Curve", col = "blue", lwd = 2)
# error rate: 0.031
```



```{r}
<<<<<<< HEAD
# Logistic Regression with improvements
=======
# Logistic Regression with Model Improvements
>>>>>>> main
train_data_c_imp <- data_c_imp[train_index, ]
test_data_c_imp <- data_c_imp[-train_index, ] 

logit_fit_2 <- glm(censoring ~ longitude + 
                   latitude + 
                   medianIncome + 
                   housingMedianAge + 
                   oceanProximity + 
                   aveRooms + 
                   aveBedrooms +
                   cityProximityScore + 
                   bedroomsPerRoom + 
                   incomePerRoom +
                   distToSF +
                   distToLA +
                   cosDirToLA +
                   cosDirToSF +
                   sinDirToLA +
                   sinDirToSF,
                 data = train_data_c_imp, 
                 family = binomial)

logit_pred_2 <- as.numeric(predict(logit_fit_2, 
                      newdata = test_data_c_imp, 
                      type = "response") > 0.5) # use 0.5 for now 

error_rate <- mean(logit_pred_2 != test_data_c_imp$censoring)

print(paste("Logistic Regression Prediction Error Rate:", round(error_rate, 3)))

#library(caret)
# Confusion matrix 
confusion_matrix <- confusionMatrix(as.factor(logit_pred_2), as.factor(test_data_c_imp$censoring))
print(confusion_matrix)

# ROC 
#library(caret)
#library(pROC)
logit_pred_probs_2 <- predict(logit_fit_2, newdata = test_data_c_imp, type = "response")
roc_curve_2 <- roc(test_data_c_imp$censoring, logit_pred_probs_2)
plot(roc_curve_2, main = "ROC Curve", col = "blue", lwd = 2)
# error rate: 0.029
```


```{r}
<<<<<<< HEAD
## KNN
=======
## KNN-CV Without Model Improvements
>>>>>>> main

# Prepare the predictors (X) and response (Y)
train.X <- model.matrix(~ longitude + 
                          latitude + 
                          housingMedianAge + 
                          aveRooms + 
                          aveBedrooms + 
                          population + 
                          medianIncome + 
                          oceanProximity, 
                        data = train_data_c)[, -1]

test.X <- model.matrix(~ longitude + 
                         latitude + 
                         housingMedianAge + 
                         aveRooms + 
                         aveBedrooms + 
                         population + 
                         medianIncome + 
                         oceanProximity, 
                       data = test_data_c)[, -1]

train.Y <- train_data_c$censoring
test.Y <- test_data_c$censoring

# Use cross-validation on the training set to choose the best k
knn_rp <- rep(0, 15)
for (k in 1:15) {
  set.seed(5)
  knn.pred.cv <- knn.cv(train = train.X, cl = train.Y, k = k)
  knn_rp[k] <- mean(knn.pred.cv == train.Y)
}

k.cv <- which.max(knn_rp)
cat("Best k selected by CV is:", k.cv, "\n")
<<<<<<< HEAD

# Fit and predict on the test set using the best k
set.seed(5)
knn_pred_c_1 <- knn(train = train.X, test = test.X, cl = train.Y, k = k.cv)

# Compute confusion matrix and error rate
confusionMatrix(as.factor(knn_pred_c_1), as.factor(test.Y))

(knn_err_c_1 = mean(knn_pred_c_1 != test.Y))
cat("The KNN error rate is", round(knn_err_c_1, 4), "\n")


#The KNN error rate is 0.049 

```
=======
>>>>>>> main

# Fit and predict on the test set using the best k
set.seed(5)
knn_pred_c_1 <- knn(train = train.X, test = test.X, cl = train.Y, k = k.cv)

# Compute confusion matrix and error rate
confusionMatrix(as.factor(knn_pred_c_1), as.factor(test.Y))

(knn_err_c_1 = mean(knn_pred_c_1 != test.Y))
cat("The KNN error rate is", round(knn_err_c_1, 4), "\n")


#The KNN error rate is 0.049 

```


```{r}
<<<<<<< HEAD
## KNN with improvements
=======
## KNN-CV with Model Improvements
>>>>>>> main

# Prepare the predictors (X) and response (Y)
train.X <- model.matrix(~ longitude + 
                          latitude + 
                          medianIncome + 
                          housingMedianAge + 
                          oceanProximity + 
                          aveRooms + 
                          aveBedrooms +
                          cityProximityScore + 
                          bedroomsPerRoom + 
                          incomePerRoom +
                          distToSF +
                          distToLA +
                          cosDirToLA +
                          cosDirToSF +
                          sinDirToLA +
                          sinDirToSF, 
                        data = train_data_c_imp)[, -1]

test.X <- model.matrix(~  longitude + 
                          latitude + 
                          medianIncome + 
                          housingMedianAge + 
                          oceanProximity + 
                          aveRooms + 
                          aveBedrooms +
                          cityProximityScore + 
                          bedroomsPerRoom + 
                          incomePerRoom +
                          distToSF +
                          distToLA +
                          cosDirToLA +
                          cosDirToSF +
                          sinDirToLA +
                          sinDirToSF, 
                       data = test_data_c_imp)[, -1]

train.Y <- train_data_c_imp$censoring
test.Y <- test_data_c_imp$censoring

# Use cross-validation on the training set to choose the best k
knn_rp <- rep(0, 15)
for (k in 1:15) {
  set.seed(5)
  knn.pred.cv <- knn.cv(train = train.X, cl = train.Y, k = k)
  knn_rp[k] <- mean(knn.pred.cv == train.Y)
}

k.cv <- which.max(knn_rp)
cat("Best k selected by CV is:", k.cv, "\n")

# Fit and predict on the test set using the best k
set.seed(5)
knn_pred_c_2 <- knn(train = train.X, test = test.X, cl = train.Y, k = k.cv)

# Compute confusion matrix and error rate
confusionMatrix(as.factor(knn_pred_c_2), as.factor(test.Y))

(knn_err_c_2 = mean(knn_pred_c_2 != test.Y))
cat("The KNN error rate is", round(knn_err_c_2, 4), "\n")


#The KNN error rate is 0.0306 

```

<<<<<<< HEAD
## 2.3.4 Justifying the better classification method


Need to update

In our 80/20 hold-out test we observed:

\begin{table}[ht]
\centering
\begin{tabular}{l c}
\hline
\textbf{Model}            & \textbf{Misclassification Rate} \\ 
\hline
Logistic Regression       & 0.0310                          \\
Linear Discriminant Analysis (LDA) & 0.0343                          \\
\hline
\end{tabular}
\caption{Test-set misclassification rates for Logistic Regression and LDA.}
\label{tab:misclass}
\end{table}

Therefore, by this test Logistic Regression performs better than LDA, since it has the lower test error rate.

\textbf{Reasoning:}
\begin{enumerate}
  \item Bias
  \begin{itemize}
    \item Logistic regression only makes one structural assumption: that the log-odds of censoring is a linear function of the predictors. Also there is only 
  \end{itemize}
  \item Variance
  \begin{itemize}
    \item
  \end{itemize}
  \item Trade-off
  \begin{itemize}
    \item
  \end{itemize}
\end{enumerate}
=======
Note: We did not generate ROC curves for KNN because it does not provide continuous probability scores but rather hard class labels. ROC analysis requires varying the classification threshold across a range of values, which is not possible with the standard KNN output. While probabilistic adaptations of KNN exist, they were not necessary here since our objective was classification accuracy, not probability estimation.

## 2.2.4 Justifying the better classification method

Logistic regression outperformed KNN-CV in our 80/20 hold-out test, with test errors of 0.0290 and 0.0306 respectively. Logistic regression is a parametric model, it assumes a specific relationship between predictors and the target, resulting in higher bias but lower variance. This rigidity works in its favour when the underlying structure of the data is close to linear, as it avoids fitting noise and reduces the risk of overfitting. KNN, on the other hand, is non-parametric and makes very few assumptions about the data’s form. It is flexible and can capture complex, non-linear patterns, but that flexibility comes at the cost of high variance, especially in high-dimensional settings.

While KNN’s flexibility can be useful, it becomes increasingly unstable in high-dimensional spaces where data points are more sparsely distributed and neighbourhoods lose their meaning. Specifically, the distance between the nearest and farthest neighbours shrinks, making it hard to define what counts as “close,” which undermines the core logic of KNN. Even with cross-validation to choose the optimal k, KNN was still overfitting, likely due to this dimensionality issue. Logistic regression, despite its linear constraint, handled the complexity of the data more gracefully by not reacting too much to the noise.

The lower test error achieved by logistic regression shows that its higher bias was actually beneficial here, it provided a smoother, more stable model that generalised well. So while KNN theoretically offers more flexibility, in this case it became a liability, and logistic regression ultimately struck the better balance. To note, the test error is only marginally larger for the KNN, and both are relatively low so it is clear that both models are effective, just for this random seed logistic regression is the winner.


>>>>>>> main

# 2.3 A hybrid approach

## 2.3.1 Dicussing feasibility of the approach
<<<<<<< HEAD
Estimate using improved KNN and then apply the censoring from logistic.
More explanation necessary
=======
To address the censoring issue in the dataset, a hybrid approach that combines regression and classification models is both feasible and potentially very effective. The best regression model from 2.1.3 is used to predict house values, while the best classifier from 2.2.2 identifies whether a given observation is censored; i.e. whether the true house value exceeds the $500,001 threshold. This setup allows us to adjust regression predictions when the classifier indicates censoring, helping to reduce the bias that occurs when censored values are treated as if they were exact rather than right censored.

The procedure involves four main steps. First, we use the KNN regression model to predict house prices. Second, a logistic regression classifier estimates the probability of censoring. Third, we adjust the regression predictions by setting them to 500001 for observations predicted to be censored. Finally, we evaluate the performance of this adjusted model using test MSE. This combined method leverages the strengths of both models and is a practical way to improve prediction accuracy in the presence of censoring.
>>>>>>> main

## 2.3.2 test MSE of medianHousingValue using the approach

```{r}
# Step 1: Predict house prices with KNN regression
knn_pred_2 <- predict(knn_reg_model_2, newdata = test_data_encoded)

# Step 2: Predict censoring with logistic regression
logit_pred_2 <- as.numeric(predict(logit_fit_2, newdata = test_data_c_imp, type = "response") > 0.5)

# Step 3: Adjust predicted house prices
# If censored predicted, clip to 500001
adjusted_preds <- ifelse(logit_pred_2 == 1, 500001, knn_pred_2)

# Step 4: Compute MSE using all observations
final_mse <- mean((adjusted_preds - test_data_encoded$medianHouseValue)^2)

cat("Final Test MSE:", round(final_mse, 2), "\n")


# Final Test MSE: 3025628814 
# this is worse then KNN_2
```

## 2.3.3 Comparison of the accuracy of this procedure to model in 2.1.3

<<<<<<< HEAD
No test MSE for combined model is worse.

Other note, random test/train sample too. 

The combined model clips predictions to 500001 when the logistic model predicts censoring, but if that prediction is wrong, it forces the value far from the true (uncensored) price, increasing error. KNN, on the other hand, always predicts a value — even for censored cases — and sometimes lands closer to the unknown true price (which is at least 500001), purely by chance. This makes KNN appear to perform better in terms of MSE, even though it doesn't account for censoring and might be underestimating the true values.
=======
The hybrid model resulted in a slightly higher test MSE (3,025,628,814) compared to the original KNN regression model from 2.1.3 (2,999,761,722). These results were obtained using a random 80/20 train-test split of the data the same split used throughout the course of this report. Given how close the two MSE values are, it's possible that on a different random sample, the hybrid model could perform slightly better. Overall, the difference is minimal, suggesting that both models offer comparable predictive performance in this context.

The reason the hybrid model does not consistently outperform KNN likely stems from how it handles censoring using a hard classification decision. Specifically, it clips predicted values to 500001 whenever the logistic classifier predicts an observation is censored. This is because logistic regression applies a strict decision boundary (at 0.5) to make this call, which introduces a sharp threshold with no gradual transition. This can be problematic near the boundary, where small changes in input may flip the classification, yet the model treats the output as definitively censored or not. If the classifier incorrectly predicts censoring, the hard cap forces the prediction to 500001, which can be significantly off from the true (uncensored) value. In contrast, the KNN model always outputs a continuous estimate. When the classifier misclassifies an uncensored point as censored, the true value is often still closer to the KNN prediction than to the fixed 500001. As a result, KNN may show better MSE performance, despite not explicitly addressing censoring.
>>>>>>> main
