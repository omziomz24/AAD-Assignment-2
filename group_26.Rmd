---
title: "Group-26 AAD-Assignment-2"
author: "Omar, Eloise, Alina, Sue"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: 4
    number_sections: false
    fig_width: 8
    fig_height: 6
    keep_tex: true
  html_document:
    toc: true
    toc_depth: '4'
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      dev = "png",
                      dpi = 150)

# Load relevant libraries
library(dplyr)
library(ggplot2)
```

# 2.1 Nonlinear model vs non-parametric model

\noindent \large \textbf{GAM:} \normalsize

\noindent \textbf{Pros}
\begin{itemize}
  \item \textbf{Flexibility to capture non-linearity}. In a GAMM we replace each linear term $\beta_{k} x_{k}$ with a smooth function $f_{k}(x_{k})$,     meaning we can automatically model non-linear relationships between each predictor and the response without having to manually try out different      transformation on each variable individually.
  \item \textbf{Non-linear fits may be more accurate}. The ability of creating this non-linear predictor and response relationship may make our model     more accurate when predicting the medianHousingValue.
  \item \textbf{Interperability from additivity}. Since a GAMM is additive, we can examine the effect of each $X_j$ on $Y$ individually while holding      all of the other variables fixed. Therefore, we can understand each variables individual effect on house value.
  \item \textbf{Control over smoothness}. Each $f_j$ comes with an associated smoothing parameter (or degrees of freedom), making it straightforward      to trade bias and variance  (e.g. via cross-validation).
\end{itemize}

\noindent \textbf{Cons}
\begin{itemize}
  \item \textbf{Additivity assumption restriction}. If there are strong synergistic effects like between location (longitude/latitude) and income         \textendash an additive model will not capture them unless explicit interaction terms $X_j \times X_k$ are included or low-dimensional               interactions function of the form $f_{jk}(X_j, X_k)$ are manually introduced. 
  \item \textbf{Computational cost}. Fitting this model to over \textasciitilde$20,600$ observations and multiple smoothers can be very slow when        compared to fitting a single parametric method.
\end{itemize}

\noindent \large \textbf{KNN:} \normalsize

\noindent \textbf{Pros}
\begin{itemize}
  \item \textbf{Completely non-parametric}. KNN makes no assumptions about the form of $f(X)$, allowing the model to potentially fit better than a      parametric model. At a point $x_0$ KNN averages the responses of the $K$ closest training blocks: $\hat{f}(x_{0}) = \frac{1}{K} \sum_{x_{i}           \in N_{0}}{y_i}$, where $N_0$ is the set of the $K$ nearest neighbors.
  \item \textbf{Control of bias-variance}. A small $K$ yields a very flexible, low-bias but high-variance fit; large $K$ yields a smoother,      lower-variance fit.
\end{itemize}

\noindent \textbf{Cons}
\begin{itemize}
  \item \textbf{Dimensoniality constraints}. As the number of predictors grows, the "nearest" neighbors tend to be far away in a high-dimensional       space, so KNN's performance degrades rapidly as the number of predictors grow.
  \item \textbf{Distance metric sensitivity}. With a KNN you must scale the numeric features and encode the categorical features (e.g.                  oceanProximity) carefully. Otherwise poorly scaled or encoded features can dominate the distance of the nearest neighbors calculation.
  \item \textbf{Computationally intensive with predictions}. For each new group of predictions all \textasciitilde$20,600$ must be computed to find     the $K$ nearest, which can be very intensive and slow.
  \item \textbf{Low interperability}. There is no simple way to explain a KNN prediction beyond pointing to the raw neighbors and their average.
\end{itemize}

\noindent \large \textbf{In this housing-price context: } \normalsize
\begin{itemize}
  \item \textbf{GAMM:} it is likely to give an interpretable model with, as we can analyse the partial-effects (e.g. how median income or ocean         proximity individually affects price), and we can capture smooth non-linear trends.
  \item \textbf{KNN:} can capture complex interactions, between predictors, automatically, but with eight predictors (including a categorical     one)   it may run into high-dimensionality issues, making distance-based averaging unstable and slow.
\end{itemize} 


## 2.1.1 GAM vs KNN approach

## 2.1.2 GAM vs KNN Regression Model

```{r}
# install.packages(c("caret", "tidyverse"))
library(caret)
library(tidyverse)

# View(data)
data <- read.csv("Assignt2_data.csv", header = TRUE)
data <- na.omit(data)

# separate the train and test set out 
set.seed(1)
train_index <- createDataPartition(data$medianHouseValue, p = 0.8, list = FALSE)
train_data <- data[train_index, ]
test_data  <- data[-train_index, ]

# train the knn regression model
  # model improvements including pca and cv for k selection 
knn_reg_model <- train(
  medianHouseValue ~ ., 
  data = train_data,
  method = "knn",
  preProcess = c("center", "scale", "pca"),  # scales & applies PCA 
  tuneGrid = data.frame(k = 1:10),  # test k = 1 to 10
  trControl = trainControl(method = "cv", number = 5) 
) # takes a while to run 

# best k 
print(knn_reg_model)

# KNN MSE calculation 
knn_pred <- predict(knn_reg_model, newdata = test_data)
knn_MSE <- mean((knn_pred - test_data$medianHouseValue)^2) 
cat("The KNN MSE is", knn_MSE, "\n")
```

```{r}
# GAM 


# install.packages("mgcv")
library(mgcv)

# the split is the same as before 

# train the GAM 
gam_model <- gam(medianHouseValue ~ s(latitude) + s(longitude) + 
                   s(housingMedianAge) + s(aveRooms) +
                   s(aveBedrooms) + s(population) +
                   s(medianIncome), data = train_data)

# Calculate MSE 
gam_pred <- predict(gam_model, newdata = test_data)
gam_MSE <- mean((gam_pred - test_data$medianHouseValue)^2)
cat("The GAM MSE is", gam_MSE, "\n")
```

```{r}
# GAM improvements 

gam.check(gam_model) # 
summary(gam_model)

# suspect high corr between longitude and latitude 
 # should add interaction term 
plot(data$longitude, data$latitude)

# same thing with aveRooms and aveBedrooms 
plot(data$aveRooms, data$aveBedrooms)
```


```{r} 
# Using the predictors from Asmt1 ? 

# bedroomsPerRoom
data$bedroomsPerRoom <- data$aveBedrooms / data$aveRooms

# distance to centre
center_lat <- mean(data$latitude)
center_lon <- mean(data$longitude)
# data$distToCenter <- sqrt((data$latitude - center_lat)^2 + (data$longitude - center_lon)^2)

# incomePerRoom
data$incomePerRoom = data$medianIncome / data$aveRooms

# Dist from LA and SF
library(geosphere)
la_coords <- c(-118.24, 34.05)   # (longitude, latitude)
sf_coords <- c(-122.42, 37.77)
distToLA <- apply(data[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, la_coords) / 1000  # convert meters to km
}) # Add distance to LA
distToSF <- apply(data[, c("longitude", "latitude")], 1, function(coord) {
  distGeo(coord, sf_coords) / 1000
}) # Add distance to SF
# Compute direction angles
dirToLA <- atan2(data$latitude - la_coords[2], data$longitude - la_coords[1])
dirToSF <- atan2(data$latitude - sf_coords[2], data$longitude - sf_coords[1])

# City Proximity 
cosDirToLA <- cos(dirToLA)
sinDirToLA <- sin(dirToLA)
cosDirToSF <- cos(dirToSF)
sinDirToSF <- sin(dirToSF)
data$cityProximityScore <- 1 / (1 + distToLA) + 1 / (1 + distToSF)

# View(data)
```

```{r}
library(mgcv)
train_data = data[train_index, ]
gam_model_2 <- gam(medianHouseValue ~ s(latitude) + 
                     s(longitude) + 
                     s(housingMedianAge) + 
                     s(aveRooms) +
                     s(aveBedrooms) + 
                     s(population) + 
                     s(medianIncome) + 
                     factor(oceanProximity) + 
                     te(longitude, latitude) + 
                     s(cityProximityScore) + 
                     s(bedroomsPerRoom) + 
                     s(incomePerRoom) 
                     ,  
                   data = train_data,
                   select = TRUE) # to reduce overfitting 


# Calculate MSE 
test_data = data[-train_index, ]
gam_pred_2 <- predict(gam_model_2, newdata = test_data)
gam_MSE_2 <- mean((gam_pred_2 - test_data$medianHouseValue)^2)
cat("The GAM MSE (imoroved) is", gam_MSE_2, "\n")
```







## 2.1.3 Which model performs better?



# 2.2 Classification models

## 2.2.1 Two classification methods

## 2.2.2 Suggested classifiers

## 2.2.3 Which classification method performs better?

# 2.3 A hybrid approach

## 2.3.1 Dicussing feasibility of the approach

## 2.3.2 test MSE of medianHousingValue using the approach

## 2.3.3 Comparison of the accuracy of this procedure to model in 2.1.3


